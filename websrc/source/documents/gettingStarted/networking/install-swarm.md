---
layout: "documents"
page_title: "Swarm Cluster"
sidebar_current: "getting-started-networking-installation-swarm"
description: |-
  Setting up a Swarm Cluster.
---

# Installing Contiv Network with Docker Swarm

### Pre-requisites
1. You need Ubuntu 15 or Centos 7 on each of your servers used for the Contiv cluster

2. Do `export CLUSTER_NODE_IPS=ips of all nodes in cluster`, and `export no_proxy=ips of all nodes in cluster,127.0.0.1,localhost,netmaster` in your shell

3. If your servers are behind an http proxy (usually the case in many cisco labs...), you need
   to do `export http_proxy=<proxy url>` and  `export https_proxy=<proxy_url>` in your shell

4. You will select and use on server to initiate installation on all servers in the cluster.
   Please refrain from running install from multiple servers. Instead, stick to the same server to initiate
   installation.

5. It is recommended that you enable passwordless SSH access from the selected server to all
   the other servers in the cluster.
   An example of how to set this up is [here](http://www.linuxproblem.org/art_9.html)

6. It is recommended that you enable passwordless sudo on all servers as well.
   Example set up instructions can be found [here](http://askubuntu.com/questions/192050/how-to-run-sudo-command-with-no-password)

7. Get the IP addresses (DNS names work as well) of all the servers and the network interface on which this IP address is configured

8. Verify Python is installed on the target machines

### Step 1: Download the installer script
Log into one of the servers and download the installer script using the following command:

```
wget https://raw.githubusercontent.com/contiv/demo/master/net/net_demo_installer
```

Note that if you are behind a proxy, you may need to set `https_proxy` environment variable
for it to work.

### Step 2: Setup cfg.yml
Create the configuration file and provide information about each server's reachability.
Sample configuration file can be found here: [sample_cfg.yml](extras/sample_cfg.yml)

#### Information in cfg.yml

`CONNECTION_INFO`:
    This is a mandatory option that provides the access information to all server-nodes in the setup.

For every server in the setup, provide the IP/DNS and the control, data interface

```
    CONNECTION_INFO:
        <server1-ip-or-dns>:
            control: <interface on which control protocols can interact>
            data: <interface used to send data packets>
        <server2-ip-or-dns>:
            control: <interface on which control protocols can interact>
            data: <interface used to send data packets>
```

### Step 3: Provide executable privileges and run installer script

Run net_demo_installer script.

```
    chmod +x net_demo_installer
    ./net_demo_installer
```

set the environment variable contiv_network_version if a different version of Contiv network is needed.

```
export contiv_network_version="v0.1-06-17-2016.08-42-14.UTC"
```

contiv network version can be obtained from: https://github.com/contiv/netplugin/releases

"aci_gw_image" specifies the docker image used for the aci-gw. This defaults to "contiv/aci-gw:latest".

If your setup requires a different image, just set this "aci_gw_image" environment variable to the "image-name:version".

e.g:

```
export aci_gw_image="contiv/aci-gw:v2"
```

Will fetch container aci-gw of contiv user from docker hub registry with v2 tag.

Run net_demo_installer script.

```
./net_demo_installer
```

##### NOTE:
- To restart the services already deployed, run the installer with -r option. This ensures that the services are restarted in a clean state.

```
            ./net_demo_installer -r
```

- The installer script will ask for username/password if passwordless ssh is not set during the installation
- Running the installer with '-c' option clears up any files that are auto-generated by the script

### Step 4: Use netctl commands to demo Contiv networking
Follow steps in [docs.contiv.io](http://docs.contiv.io) netplugin section for details on using netctl to explore various features of Contiv networking

### Under the hoods
The installer script performs the following actions:
- performs preliminary checks to verify that the supported version of OS is installed on the servers
- verifies that access to the servers can be established
- creates the ansible inventory file
- establishes variables necessary for the servers to be provisioned in the appropriate mode
- runs the ansible playbook which installs necessary packages and brings up the services

### Troubleshooting
The current limitations of the script are as follows:
- The installer script is assumed to run from one of the server nodes in the cluster. This approach ensures that the required packages are installed only on the necessary nodes.
- Connections to the servers are assumed to be on the default ssh port and the default username used is the local hostname

The script generates many files for bookkeeping during the installation procedure.
These files can be found under .gen folder in your installer directory.
If you need to clear these files and start from a clean slate, use could use the following command:

```
        ./net_demo_installer -c
```
This will list the files that will be cleared up and prompt you for confirmation to proceed.

If you find any other issues or have suggestions for improvement, please feel free to suggest/contribute.
