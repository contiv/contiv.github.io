<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Getting Started">

    <link rel="shortcut icon" href="/assets/images/favicon.png">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script>
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
    <title>Getting Started - Contiv</title>

    <link href="/assets/stylesheets/application-2b39eabd.css" rel="stylesheet"/>

    <!--[if lt IE 9]><script src="/assets/javascripts/html5shiv-099439cc.js"></script><script src="/assets/javascripts/respond.min-1eb35b04.js"></script><![endif]-->

    
  </head>

  <body id="page-Getting Started" class=" page-Getting Started layout-documents page-sub">

<div id="header" class="navbar-static-top">
  <div class="container">
    <div class="row">
      
        <div class="navbar-header">
          <div class="navbar-brand">
              <a class="logo" href="/">Contiv</a>
            </a>
          </div>
          
        </div>
        <div class="buttons hidden-xs">
          <nav role="navigation">
            <ul class="main-links nav navbar-nav navbar-left">
              <li class="first li-under"><a href="/documents/index.html">Documentation</a></li>
              <li class="li-under"><a href="/articles/index.html">Articles</a></li>
              <li class="li-under"><a href="https://github.com/contiv" target="_blank">
                        GitHub
                        <i class="fa fa-github fa-lg"></i>
                        </a></li>
                    <li class="li-under"><a href="https://contiv.herokuapp.com" target="_blank">
                        Slack
                        <i class="fa fa-slack fa-lg"></i>
                        </a></li>
                    <li><a href="https://twitter.com/projectcontiv" target="_blank">
                        Twitter
                        <i class="fa fa-twitter fa-lg"></i>
                        </a></li>
            </ul>
          </nav>
        </div>
      
    </div>
  </div>
</div>


<div class="sidebar-overlay"></div>


<aside id="sidebar" class="sidebar sidebar-default sidebar-fixed-right" role="navigation">
    
    <div class="sidebar-header header-cover">
        
        <div class="sidebar-image">
            <img src="/assets/images/logo3-a96fcb82.png" width="49px" height="56px">
        </div>
    </div>

    
    <ul class="main nav sidebar-nav">
        <li class="first"><a href="/intro/index.html">Intro</a></li>
        <li class=""><a href="/docs/index.html">Docs</a></li>
    </ul>
    <div class="divider"></div>
    
    <ul class="external nav sidebar-nav">
        <li class=""><a class="v-btn gray sml" href="https://github.com/contiv"><svg id="svg-download" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 14" style="enable-background:new 0 0 14 14;">
  <style>
  </style>
  <path class="" d="M13,0H1C0.5,0,0,0.5,0,1v12c0,0.5,0.5,1,1,1h4.7c0,0,0,0,0-0.1c0-0.2,0-0.6,0-1.1c-1.8,0.4-2.2-0.9-2.2-0.9
    c-0.3-0.8-0.7-1-0.7-1c-0.6-0.4,0-0.4,0-0.4c0.7,0,1,0.7,1,0.7c0.6,1,1.5,0.7,1.9,0.5c0.1-0.4,0.2-0.7,0.4-0.9c-1.5-0.2-3-0.7-3-3.2
    c0-0.7,0.3-1.3,0.7-1.8C3.7,5.8,3.5,5.1,3.9,4.2c0,0,0.6-0.2,1.8,0.7c0.5-0.1,1.1-0.2,1.6-0.2c0.6,0,1.1,0.1,1.6,0.2
    c1.3-0.8,1.8-0.7,1.8-0.7c0.4,0.9,0.1,1.6,0.1,1.7c0.4,0.5,0.7,1,0.7,1.8c0,2.5-1.5,3.1-3,3.2C8.7,11.1,9,11.5,9,12.1
    c0,0.9,0,1.6,0,1.8c0,0,0,0,0,0.1h4c0.5,0,1-0.5,1-1V1C14,0.5,13.5,0,13,0z"/>
</svg>
GitHub</a></li>
    </ul>
</aside>


<div class="container">
	<div class="row">
		<div class="col-md-3 col-sm-3 col-xs-12">
					<div class="docs-sidebar hidden-print affix-top" role="complementary">
			<hr>
			<h3>User Guide</h3>
			<ul class="nav docs-sidenav">
				<li class="active">
					<a href="/documents/gettingStarted/index.html">Getting Started</a>
					<ul class="nav">
						<li>
							<a class="hasChildren" href="/documents/gettingStarted/networking/index.html">Networking</a>
							<ul class="nav">
								<li>
									<a class="hasChildren" href="/documents/gettingStarted/networking/index.html#UsingVagrant">Single Machine</a>
									<ul class="subnav">
										<li>
												<a href="/documents/gettingStarted/networking/swarm.html">Docker Swarm</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/k8s.html">Kubernetes</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/mesos.html">Mesos</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/nomad.html">Nomad</a>
										</li>
									</ul>
								</li>
								<li>
									<a class="hasChildren" href="/documents/gettingStarted/networking/index.html#WithNetworkedServers">Multiple Servers</a>
									<ul class="subnav">
										<li>
												<a href="/documents/gettingStarted/networking/install-swarm.html">Docker Swarm</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/install-k8s.html">Kubernetes</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/bgp.html">BGP</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/aci.html">Cisco ACI</a>
										</li>
									</ul>
								</li>
							</ul>
						</li>
						<li>
							<a href="/documents/gettingStarted/storage/storage.html">Storage </a>
						</li>
						<li class="active">
							<a class="hasChildren" href="/documents/gettingStarted/cluster/index.html">Cluster </a>
							<ul class="subnav">
								<li>
										<a href="/documents/gettingStarted/cluster/vagrant.html">Single Machine</a>
								</li>
								<li class="active">
										<a href="/documents/gettingStarted/cluster/baremetal.html">Multiple Servers</a>
								</li>
							</ul>
						</li>
					</ul>
				</li>
				<li>
					<a href="/documents/networking/index.html">Networking</a>
					<ul class="nav">
						<li>
								<a href="/documents/networking/features.html">Features</a>
						</li>
						<li>
								<a href="/documents/networking/concepts.html">Concepts and Terminology</a>
						</li>
						<li>
								<a href="/documents/networking/policies.html">Policies</a>
						</li>
						<li>
								<a href="/documents/networking/services.html">Service Loadbalancing</a>
						</li>
						<li>
								<a class="hasChildren" href="/documents/networking/physical-networks.html">Physical Networks</a>
								<ul class="nav">
									<li>
											<a href="/documents/networking/bgp.html">L3 Routed Networks</a>
									</li>
									<li>
											<a href="/documents/networking/l2-vlan.html">L2 Bridged Networks</a>
									</li>
									<li>
											<a href="/documents/networking/aci_ug.html">Cisco ACI</a>
									</li>
								</ul>
						</li>
						<li>
								<a href="/documents/networking/ipam.html">IPAM and Service Discovery</a>
						</li>
						<li>
								<a href="/documents/networking/ipv6.html">IPv6</a>
						</li>
					</ul>
				</li>
				<li>
					<a href="/documents/storage/index.html">Storage</a>
					<ul class="nav">
						<li>
								<a href="/documents/storage/architecture.html">Architecture</a>
						</li>
						<li>
								<a href="/documents/storage/configuration.html">Configuration</a>
						</li>
						<li>
								<a href="/documents/storage/containers.html">Creating a Container with a Volume</a>
						</li>
						<li>
								<a href="/documents/storage/volcli.html">Volcli Reference</a>
						</li>
					</ul>
				</li>
				<li>
					<a href="/documents/cluster/index.html">Cluster</a>
					<ul class="nav">
						<li>
								<a href="/documents/cluster/concepts.html">Concepts and Terminology</a>
						</li>
						<li>
								<a href="/documents/cluster/node-lifecycle.html">Node Lifecycle Management</a>
						</li>
					</ul>
				</li>
			</ul>
			<hr><p>
			<h3>Tutorials and Talks</h3>
				<ul class="nav docs-sidenav">
					<li>
						<a href="/documents/tutorials/index.html">Tutorials</a>
						<ul class="nav">
							<li>
									<a href="/documents/tutorials/container-101.html">Container Networking Tutorial</a>
							</li>
							<li>
									<a href="/documents/tutorials/contiv-compose.html">Policies with Networking</a>
							</li>
						</ul>
					</li>
					<li>
						<a href="/documents/demos/index.html">Demonstration Videos</a>
					</li>
					<li>
						<a href="/documents/talks/index.html">Community Talks</a>
					</li>
				</ul>
			<hr><p>
			<h3>Examples</h3>
				<ul class="nav docs-sidenav">
					<ul class="subnav">
						<li>
							<a href="/documents/samples/index.html">Docker Compose Examples</a>
						</li>
						<li>
							<a href="/documents/samples/index.html">Kubernetes Examples</a>
						</li>
						<li>
							<a href="/documents/samples/mcast.html">Multicast Examples</a>
						</li>
						<li>
							<a href="/documents/samples/nomad.html">Nomad Examples</a>
						</li>
					</ul>
				</ul>
			<hr><p>
			<h3>Reference</h3>
				<ul class="nav docs-sidenav">
					<ul class="subnav">
						<li><a href="https://godoc.org/github.com/contiv/contivmodel/client" target="_blank">Network API Reference</a><br></li>
						<li><a href="https://godoc.org/github.com/contiv/volplugin/config" target="_blank">Storage API Reference</a><br></li>
						<li><a href="https://godoc.org/github.com/contiv/cluster" target="_blank">Cluster API Reference</a><br></li>
					</ul>
				</ul>
			<hr><p>
		</div>

		</div>  
		<div id="main-content" class="col-sm-9 col-md-9 col-xs-12" role="main">
			<div class="bs-docs-section">
				
	<h1>Installing Contiv Cluster on a Baremetal Server or VM</h1>
<p>This page describes installing Contiv Cluster on one or more baremetal servers or virtual machines (VMs).</p>
<p><em>Note:</em> These steps have been validated on RHEL 7.2 and CentOS 7.2.</p>

<h2>Prerequisites</h2>
<p>Do the following before installing the Contiv Cluster packages:</p>

<ul>
<li>Choose one of the nodes in your cluster from which to perform the installation. The resto of these instructions refer to this node as the <em>control host</em>. 
Unless stated otherwise, all the steps below are performed on the control host.
</li>
<li>Create a user with admin privileges on the control host. The rest of these instructions refer to this login as <em>cluster-admin</em>.

<ul>
<li>The <em>cluster-admin</em> login can be an existing login.
</li>
<li>The <em>cluster-admin</em> login must have <em>passwordless sudo</em> access configured. You can use the <a name="visudo"/><a href="#visudo"><code>visudo</code></a> tool to configure passwordless access.
</li>
</ul>
</li>
<li>Generate an <em>ssh</em> key for <em>cluster-admin</em>. You can use the <a name="ssh_keygen"/><a href="#ssh_keygen"><code>ssh-keygen</code></a> tool to generate an <em>ssh</em> key.
</li>
<li>Add the public key for <em>cluster-admin</em> to all the nodes (including the control node) in your setup. 
You can use <a name="ssh_copy_id_cluster_admin_lt_hostname_gt_"/><a href="#ssh_copy_id_cluster_admin_lt_hostname_gt_"><code>ssh-copy-id cluster-admin@&lt;hostname&gt;</code></a> on each node, where <code>&lt;hostname&gt;</code> is the name of your control node.
</li>
</ul>
<p>Install the following packages on your control host.</p>

<ul>
<li>Ansible 2.0 or later
</li>
<li>Git
</li>
</ul>

<h2>Installing the Cluster Manager</h2>

<h3>Step 1. Download and Install Cluster Manager</h3>
<p>Log into your control node as <em>cluster-admin</em>, then use the following commands to:</p>
<p>1. Download Ansible</p>
<p>2. Create an inventory file called <code>hosts</code> in the <code>tmp</code> directory</p>
<p>3. Install the cluster manager service</p>
<pre class="highlight plaintext"><code>git clone https://github.com/contiv/ansible.git
cd ansible

echo [cluster-control] &gt; /tmp/hosts
echo node1 ansible_host=127.0.0.1 &gt;&gt; /tmp/hosts

ansible-playbook --key-file=~/.ssh/id_rsa -i /tmp/hosts -e '{"env": {}, "control_interface": "ifname"}' ./site.yml
</code></pre>

<ul>
<li>The <a name="control_interface"/><a href="#control_interface"><code>control_interface</code></a> is the net device that carries <em>Serf</em> traffic on this node. Subtitute the interface name for <code>ifname</code>.
</li>
<li>The <a name="env"/><a href="#env"><code>env</code></a> is the environment for running Ansible tasks such as <code>http-proxy</code>. 
</li>
</ul>
<p>If there is no special environment to be configured then set <code>control_interface</code> to an empty dictionary as shown in the example.</p>

<h3>Step 2. Configure the Cluster Manager Service</h3>
<p>Edit the cluster manager configuration file <code>/etc/default/clusterm/clusterm.conf</code> to set up the user and playbook location information. 
Following is a sample.</p>
<pre class="highlight plaintext"><code># cat /etc/default/clusterm/clusterm.conf
{
    "ansible": {
        "playbook_location": "/home/cluster-admin/ansible/",
        "user": "cluster-admin",
        "priv_key_file": "/home/cluster-admin/.ssh/id_rsa"
    }
}
</code></pre>

<ul>
<li>The <a name="playbook_location"/><a href="#playbook_location"><code>playbook-location</code></a> must contain the path of th Ansible directory you cloned 
</li>
<li>The <a name="user"/><a href="#user"><code>user</code></a> must contain the name of the <em>cluster-admin</em> user 
</li>
<li>The <a name="priv_key_file"/><a href="#priv_key_file"><code>priv_key_file</code></a> value is the location of the <code>id_rsa</code> file of the <code>cluster-admin</code> user
</li>
</ul>
<p>After you have made the changes, restart the control host:</p>
<pre class="highlight plaintext"><code>sudo systemctl restart clusterm
</code></pre>

<h2>Managing Node Lifecycle</h2>
<p>Log into the control host to manage the cluster.</p>
<p><em>Note:</em> The control host has two additional services that are not on other nodes, <code>collins</code> and <code>clusterm</code>.</p>

<ul>
<li>The <a name="collins"/><a href="#collins"><code>collins</code></a> service is for node lifecycle management and event logging. 
</li>
<li>The <a name="clusterm"/><a href="#clusterm"><code>clusterm</code></a> service is the cluster manager daemon. The <code>clusterctl</code> utility is provided to exercise the REST endpoint provided by the cluster manager.
</li>
</ul>
<p><a name="provisioning"></a></p>

<h3>Provision Additional Nodes</h3>
<p>Cluster Manager uses <em>Serf</em> as a discovery service for node health monitoring and for cluster bootstrapping.</p>
<p>Use the following command to make more nodes for the cluster availble through discovery:</p>
<pre class="highlight plaintext"><code>clusterctl discover &lt;host-ip&gt;
</code></pre>
<p>The <code>&lt;host-ip&gt;</code> should be one or more IP addresses from the management network that the nodes are on.
The management network is used only by infrastructure services such as <em>serf</em>, <em>etcd</em>, <em>swarm</em>, and so on.</p>
<p>The following command provisions the other two VMs (<code>cluster-node2</code> and <code>cluster-node3</code>) 
in the Vagrant setup for <em>Serf</em>-based discovery:</p>
<pre class="highlight plaintext"><code>clusterctl discover 192.168.2.11 192.168.2.12 --extra-vars='{"env" : {}, "control_interface": "eth1" }'
</code></pre>
<p>You must specify the <code>env</code> and <code>control_interface</code> Ansible variables. Do this by using the 
<code>--extra-vars</code> flag as shown.</p>
<p>You can also set variables as <a href="#set_get_global_vars">global variables</a>.</p>
<p>For more information on Ansible variables, see the list of <a href="#ansible_vars">Ansible Variables</a>.</p>

<h3>List Discovered Nodes</h3>
<p>Use the following command to view the cluster nodes:</p>
<pre class="highlight plaintext"><code>clusterctl nodes get
</code></pre>
<p><em>Note</em>: It takes a few minutes for the nodes to be discovered and to appear in the list.</p>
<p>To fetch information about a single node use:
<code>clusterctl node get &lt;node-name&gt;</code>.</p>
<p><a name="set_get_global_vars"></a></p>

<h3>Set and Get Global Variables</h3>
<p>You can set variables common to all cluster nodes (for example,  environment, scheduler-provider and so on) 
by using the 
<code>--extra-vars</code> flag with <code>clusterctl global set</code> command, as follows:</p>
<pre class="highlight plaintext"><code>clusterctl global set --extra-vars=&lt;vars&gt;
</code></pre>
<p>Note the following:</p>

<ul>
<li>The variables set at global level are merged with the variables specified at the node level. In case of a conflict, the node-level variable takes precedence.
</li>
<li>See <a href="#ansible_vars">Ansible Variables</a> for a list of applicable variables.
</li>
<li>The variables must be passed as a quoted JSON string using the <a name="_extra_vars"/><a href="#_extra_vars"><code>--extra-vars</code></a> flag. For example:
</li>
</ul>
<pre class="highlight plaintext"><code>clusterctl global set --extra-vars='{"env" : {"http_proxy": "my.proxy.url"}, "scheduler_provider": "ucp-swarm"}'
</code></pre>

<h3>Commission a Node</h3>
<p>To commission a node, use the following command:</p>
<pre class="highlight plaintext"><code>clusterctl node commission &lt;node-name&gt; --host-group=&lt;service-master|service-worker&gt;
</code></pre>
<p>The command pushes the configuration to the node and starts infra services on that node using <code>ansible</code>-based configuration management.</p>
<p>The services that are configured depend on the mandatory parameter <code>--host-group</code>.</p>
<p>See the <code>service-master</code> and <code>service-worker</code> host-groups in <a href="/extras/site.yml">ansible/site.yml</a> to learn more about the services that are configured.</p>
<p>To quickly check if commissioning a node worked, use:</p>
<p><code>etcdctl member list</code></p>
<p>The command lists all the commissioned members of the list.</p>
<p>Some Ansible variables are mandatory when commissioning a node. Mandatory variables, as well as some useful optional variables, are listed at <a href="#ansible_vars">Ansible Variables</a>.</p>
<p>The variables must be passed as a quoted JSON string using the <code>--extra-vars</code> flag. For example:</p>
<pre class="highlight plaintext"><code>clusterctl node commission node1 --extra-vars='{"env" : {}, "control_interface": "eth1", "netplugin_if": "eth2" }' --host-group "service-master"
</code></pre>
<p>A common set of variables (for example, environment variables) can be set just once as <a href="#set_get_global_vars">global variables</a>.
This eliminates the need to specify the common variables for every commission command.</p>

<h3>Decommission a Node</h3>
<p>Use the following command to decommission a node:</p>
<pre class="highlight plaintext"><code>clusterctl node decommission &lt;node-name&gt;
</code></pre>
<p>The command stops and removes the infra services on that node using <code>ansible</code>-based configuration management.</p>

<h3>Perform an Upgrade</h3>
<p>Use the following command to upgrade a node:</p>
<pre class="highlight plaintext"><code>clusterctl node maintain &lt;node-name&gt;
</code></pre>
<p>The command upgrades the configuration for infra services on that node using <code>ansible</code>-based configuration management.</p>

<h3>Get Provisioning Job Status</h3>
<p>Use the following command to examine the status of another provisioning command:</p>
<pre class="highlight plaintext"><code>clusterctl job get &lt;active|last&gt;
</code></pre>
<p>Cluster management workflows, including commission, decommission and so on, involve running an Ansible playbook. 
Each such workflow run is referred to as a job. You can see the status of an active job or of the last run job using this command.</p>

<h3>Managing Multiple Nodes</h3>
<p>To perform a workflow on all nodes, or a subset of nodes, in a cluster, use the <code>clusterctl nodes</code> command as follows:</p>
<pre class="highlight plaintext"><code>clusterctl nodes commission &lt;space-separated node-names&gt;
clusterctl nodes decommission &lt;space-separated node-names&gt;
clusterctl nodes maintain &lt;space-separated node-names&gt;
</code></pre>
<p>Refer the documentation of individual commands for details about the commands&#39; effects.</p>
<p><a name="ansible_vars"></a></p>

<h2>Ansible Variables for Provisioning</h2>
<p>Following is a list of the Ansible variables that can be passed when commissioning a node, or set at a global level as described in <a href="#set_get_global_vars">Set and Get Global Variables</a>.</p>
<p>Ansible variables can also be passed at the time of setting up a node for discovery as described in <a href="#provisioning">Provisioning Additional Nodes</a>.</p>
<p>Variables specified at the global level are merged with variables specified for a node-level operation. In case of a conflict, the node-level variable takes precedence.</p>
<p>Setting a global variable that has same value across all nodes in a cluster can substantially reduce the need to specified variables at every node-level operation,
 and is a recommended practice.</p>
<p>The lists of Variables that follow are in sections, <a href="#mandatory_vars"><em>Mandatory Variables</em></a> and <a href="#optional_vars"><em>Optional Variables</em></a>. (The Optional Variables list is not comprehensive, but contains variables useful for clustering tasks.)</p>
<p><em>Mandatory Variables</em> are variables that must be set before a node can be configured.</p>
<p><em>Optional Variables</em> are variables that affect the default Ansible behavior. Examples are deploying a specific scheduler stack or a specific networking mode.</p>
<p><em>Optional Variables</em> are further organized into the following service-specific subsections:</p>

<ul>
<li><a href="#serf-based-discovery">Serf-based Discovery</a>
</li>
<li><a href="#scheduler-stack">Scheduler Stack</a>
</li>
<li><a href="#contiv-networking">Contiv Networking</a>
</li>
</ul>
<p>Several other variables are made available to provide a programmability in the Ansible plays. We encourage you to look at the 
Ansible plays on the <a href="https://github.com/contiv/cluster/tree/master/vendor/ansible">Contiv Cluster GitHub site</a>.</p>
<p><a name="mandatory_vars"></a></p>

<h3>Mandatory Variables</h3>

<ul>
<li><a name="env"/><a href="#env"><code>env</code></a> is used to set the environment variables available to Ansible tasks. A common use of this variable is to set the http-proxy information.
</li>
<li><a name="env"/><a href="#env"><code>env</code></a> is specified as a JSON dictionary.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"env"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nt">"var1"</span><span class="p">:</span><span class="w"> </span><span class="s2">"val1"</span><span class="p">,</span><span class="w"> </span><span class="nt">"http_proxy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"http://my.proxy.url"</span><span class="p">,</span><span class="w"> </span><span class="nt">"https_proxy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"http://my.proxy.url"</span><span class="w"> </span><span class="p">}}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>env</strong> should be set to an empty dictionary if no environment variables need to be set.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"env"</span><span class="p">:</span><span class="w"> </span><span class="p">{}}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>control_interface</strong> identifies the netdevice on the node that carries the traffic generated by infrastructure applications like <a name="etcd"/><a href="#etcd"><code>etcd</code></a>, <code>ceph</code> and so on.
</li>
<li><strong>control_interface</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"control_interface"</span><span class="p">:</span><span class="w"> </span><span class="s2">"eth1"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>netplugin_if</strong> identifies the netdevice on the node that carries the data traffic generated by the containers networked using the Contiv data plane.
</li>
<li><strong>netplugin_if</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"netplugin_if"</span><span class="p">:</span><span class="w"> </span><span class="s2">"eth2"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>service_vip</strong> identifies a static IP address that can be used as a virtual IP to access Contiv services.
</li>
<li><strong>service_vip</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"service_vip"</span><span class="p">:</span><span class="w"> </span><span class="s2">"192.168.2.252"</span><span class="p">}</span><span class="w">
</span></code></pre>
<p><a name="optional_vars"></a></p>

<h3>Optional Variables</h3>
<p><a name="serf-based-discovery"></a></p>

<h4>Serf-based Discovery</h4>

<ul>
<li><strong>serf_cluster_name</strong> is the name of the cluster that Serf uses to discover other peer nodes. You can use this if there are multiple clusters in the same subnet of <a name="control_interface"/><a href="#control_interface"><code>control_interface</code></a> and you would like Serf to only discover the nodes in a specific cluster.
</li>
<li><strong>serf_cluster_name</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"serf_cluster_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"cluster-prod-eng"</span><span class="p">}</span><span class="w">
</span></code></pre>
<p><a name="scheduler-stack"></a></p>

<h4>Scheduler Stack</h4>

<ul>
<li><strong>scheduler_provider</strong> identifies the scheduler stack to use. Two stacks are supported: <a name="native_swarm"/><a href="#native_swarm"><code>native-swarm</code></a> and <code>ucp-swarm</code>. The first creates a swarm cluster using the stock swarm image from Docker Hub. The seconds brings-up a UDP cluster with swarm bundled in it.
</li>
<li><strong>scheduler_provider</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"scheduler_provider"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ucp-swarm"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>ucp_bootstrap_node_name</strong> is the name (as seen in the <a name="clusterctl_nodes_get"/><a href="#clusterctl_nodes_get"><code>clusterctl nodes get</code></a> command) of the node to bootstrap UCP with. This is the first node that is commissioned in the cluster. This variable is mandatory when <strong>scheduler_provider</strong> is set to <code>ucp-swarm</code>.
</li>
<li><strong>ucp_bootstrap_node_name</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"ucp_bootstrap_node_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"cluster-node1-0"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>ucp_license_file</strong> identifies the path to the UCP license file on the host where Ansible is run. This variable can be used to pass the UCP license at the time of configuring a UCP cluster.
</li>
<li><strong>ucp_license_file</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"ucp_license_file"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/path/to/ucp/licence"</span><span class="p">}</span><span class="w">
</span></code></pre>
<p><a name="contiv-networking"></a></p>

<h4>Contiv Networking</h4>

<ul>
<li><strong>contiv_network_mode</strong> identifies the mode of operation for <a name="netplugin"/><a href="#netplugin"><code>netplugin</code></a>. Netplugin supports two modes: <code>aci</code> and <code>standalone</code>. <code>aci</code> mode is used to start <code>netplugin</code> in a Cisco APIC managed fabric deployment. &#39;standalone&#39; mode can be used when deploying <code>netplugin</code> with standalone layer2 and layer3 switches.
</li>
<li><strong>contiv_network_mode</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"contiv_network_mode"</span><span class="p">:</span><span class="w"> </span><span class="s2">"aci"</span><span class="p">}</span><span class="w">
</span></code></pre>

<h5>ACI Mode</h5>
<p>The following variables are applicable when <code>contiv_network_mode</code> is set to <code>aci</code>:</p>

<ul>
<li><strong>apic_url</strong> specifies the URL for APIC. This is a mandatory variable in <a name="aci"/><a href="#aci"><code>aci</code></a> mode.
</li>
<li><strong>apic_url</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_url"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://&lt;apic-server-url&gt;:443"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>apic_username</strong> specifies the username for APIC. This is a mandatory variable in <a name="aci"/><a href="#aci"><code>aci</code></a> mode.
</li>
<li><strong>apic_username</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_username"</span><span class="p">:</span><span class="w"> </span><span class="s2">"my-user"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>apic_password</strong> specifies the password for APIC. This is a mandatory variable in <a name="aci"/><a href="#aci"><code>aci</code></a>  mode.
</li>
<li><strong>apic_password</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_password"</span><span class="p">:</span><span class="w"> </span><span class="s2">"my-password"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>apic_leaf_nodes</strong> specifies the full path of the leaf nodes managed by APIC. This is a mandatory variable in <a name="aci"/><a href="#aci"><code>aci</code></a> mode.
</li>
<li><strong>apic_leaf_nodes</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_leaf_nodes"</span><span class="p">:</span><span class="w"> </span><span class="s2">"topology/pod-1/node-101,topology/pod-1/node-102"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>apic_phys_domain</strong> specifies the name of the physical domain name created in APIC.
</li>
<li><strong>apic_phys_domain</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_phys_domain"</span><span class="p">:</span><span class="w"> </span><span class="s2">"allVlans"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>apic_epg_bridge_domain</strong> can be optionally used to provide a pre-created bridge domain. The bridge domain must already exist under tenant <a name="common"/><a href="#common"><code>common</code></a>.
</li>
<li><strong>apic_epg_bridge_domain</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_epg_bridge_domain"</span><span class="p">:</span><span class="w"> </span><span class="s2">"my-bd"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>apic_contracts_unrestricted_mode</strong> can optionally be used to allow unrestricted communication between EPGs.
</li>
<li><strong>apic_contracts_unrestricted_mode</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_contracts_unrestricted_mode"</span><span class="p">:</span><span class="w"> </span><span class="s2">"yes"</span><span class="p">}</span><span class="w">
</span></code></pre>

<h5>Standalone Mode</h5>
<p>The following variables are applicable when <code>contiv_network_mode</code> is set to <code>standalone</code>:</p>

<ul>
<li><strong>fwd_mode</strong> specifies whether netplugin shall bridge or route the packet. Netplugin supports two forwarding modes viz. <a name="bridge"/><a href="#bridge"><code>bridge</code></a> and <code>routing</code>.
</li>
<li><strong>fwd_mode</strong> is specified as a JSON string.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"fwd_mode"</span><span class="p">:</span><span class="w"> </span><span class="s2">"routing"</span><span class="p">}</span><span class="w">
</span></code></pre>


			</div>
		</div>
	</div>
</div>