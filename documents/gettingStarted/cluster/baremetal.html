<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Getting Started">

    <link rel="shortcut icon" href="/assets/images/favicon.png">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script>
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
    <title>Getting Started - Contiv</title>

    <link href="/assets/stylesheets/application-b128c5a2.css" rel="stylesheet"/>

    <!--[if lt IE 9]><script src="/assets/javascripts/html5shiv-099439cc.js"></script><script src="/assets/javascripts/respond.min-1eb35b04.js"></script><![endif]-->

    
  </head>

  <body id="page-Getting Started" class=" page-Getting Started layout-documents page-sub">

<div id="header" class="navbar-static-top">
  <div class="container">
    <div class="row">
      
        <div class="navbar-header">
          <div class="navbar-brand">
              <a class="logo" href="/">Contiv</a>
            </a>
          </div>
          
        </div>
        <div class="buttons hidden-xs">
          <nav role="navigation">
            <ul class="main-links nav navbar-nav navbar-left">
              <li class="first li-under"><a href="/documents/index.html">Documentation</a></li>
              <li class="li-under"><a href="/articles/index.html">Articles</a></li>
              <li class="li-under"><a href="https://github.com/contiv" target="_blank">
                        GitHub
                        <i class="fa fa-github fa-lg"></i>
                        </a></li>
                    <li class="li-under"><a href="https://contiv-slack.herokuapp.com" target="_blank">
                        Slack
                        <i class="fa fa-slack fa-lg"></i>
                        </a></li>
                    <li><a href="https://twitter.com/projectcontiv" target="_blank">
                        Twitter
                        <i class="fa fa-twitter fa-lg"></i>
                        </a></li>
            </ul>
          </nav>
        </div>
      
    </div>
  </div>
</div>


<div class="sidebar-overlay"></div>


<aside id="sidebar" class="sidebar sidebar-default sidebar-fixed-right" role="navigation">
    
    <div class="sidebar-header header-cover">
        
        <div class="sidebar-image">
            <img src="/assets/images/logo3-a96fcb82.png" width="49px" height="56px">
        </div>
    </div>

    
    <ul class="main nav sidebar-nav">
        <li class="first"><a href="/intro/index.html">Intro</a></li>
        <li class=""><a href="/docs/index.html">Docs</a></li>
    </ul>
    <div class="divider"></div>
    
    <ul class="external nav sidebar-nav">
        <li class=""><a class="v-btn gray sml" href="https://github.com/contiv"><svg id="svg-download" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 14" style="enable-background:new 0 0 14 14;">
  <style>
  </style>
  <path class="" d="M13,0H1C0.5,0,0,0.5,0,1v12c0,0.5,0.5,1,1,1h4.7c0,0,0,0,0-0.1c0-0.2,0-0.6,0-1.1c-1.8,0.4-2.2-0.9-2.2-0.9
    c-0.3-0.8-0.7-1-0.7-1c-0.6-0.4,0-0.4,0-0.4c0.7,0,1,0.7,1,0.7c0.6,1,1.5,0.7,1.9,0.5c0.1-0.4,0.2-0.7,0.4-0.9c-1.5-0.2-3-0.7-3-3.2
    c0-0.7,0.3-1.3,0.7-1.8C3.7,5.8,3.5,5.1,3.9,4.2c0,0,0.6-0.2,1.8,0.7c0.5-0.1,1.1-0.2,1.6-0.2c0.6,0,1.1,0.1,1.6,0.2
    c1.3-0.8,1.8-0.7,1.8-0.7c0.4,0.9,0.1,1.6,0.1,1.7c0.4,0.5,0.7,1,0.7,1.8c0,2.5-1.5,3.1-3,3.2C8.7,11.1,9,11.5,9,12.1
    c0,0.9,0,1.6,0,1.8c0,0,0,0,0,0.1h4c0.5,0,1-0.5,1-1V1C14,0.5,13.5,0,13,0z"/>
</svg>
GitHub</a></li>
    </ul>
</aside>


<div class="container">
	<div class="row">
		<div class="col-md-3 col-sm-3 col-xs-12">
					<div class="docs-sidebar hidden-print affix-top" role="complementary">
			<hr>
			<h3>User Guide</h3>
			<ul class="nav docs-sidenav">
				<li class="active">
					<a href="/documents/gettingStarted/index.html">Getting Started</a>
					<ul class="nav">
						<li>
							<a class="hasChildren" href="/documents/gettingStarted/networking/index.html">Networking</a>
							<ul class="nav">
								<li>
									<a class="hasChildren" href="/documents/gettingStarted/networking/vagrant.html">Vagrant try out</a>
									<ul class="subnav">
										<li>
												<a href="/documents/gettingStarted/networking/swarm.html">Docker Swarm</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/k8s.html">Kubernetes</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/mesos.html">Mesos</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/nomad.html">Nomad</a>
										</li>
									</ul>
								</li>
								<li>
									<a class="hasChildren" href="/documents/gettingStarted/networking/installation.html">Netplugin Installation</a>
									<ul class="subnav">
										<li>
												<a href="/documents/gettingStarted/networking/install-swarm.html">Docker Swarm</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/install-k8s.html">Kubernetes</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/bgp.html">BGP</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/aci.html">Cisco ACI</a>
										</li>
									</ul>
								</li>
							</ul>
						</li>
						<li>
							<a href="/documents/gettingStarted/storage/storage.html">Storage </a>
						</li>
						<li class="active">
							<a class="hasChildren" href="/documents/gettingStarted/cluster/index.html">Cluster </a>
							<ul class="subnav">
								<li>
										<a href="/documents/gettingStarted/cluster/vagrant.html">Vagrant Try out</a>
								</li>
								<li class="active">
										<a href="/documents/gettingStarted/cluster/baremetal.html">Baremetal or VM installation</a>
								</li>
							</ul>
						</li>
					</ul>
				</li>
				<li>
					<a href="/documents/networking/index.html">Networking</a>
					<ul class="nav">
						<li>
								<a href="/documents/networking/concepts.html">Concepts and Terminology</a>
						</li>
						<li>
								<a href="/documents/networking/policies.html">Policies</a>
						</li>
						<li>
								<a href="/documents/networking/services.html">Service Routing</a>
						</li>
						<li>
								<a class="hasChildren" href="/documents/networking/physical-networks.html">Physical Networks</a>
								<ul class="nav">
									<li>
											<a href="/documents/networking/bgp.html">L3 Routed Networks</a>
									</li>
									<li>
											<a href="/documents/networking/l2-vlan.html">L2 Bridged Networks</a>
									</li>
									<li>
											<a href="/documents/networking/aci_ug.html">Cisco ACI</a>
									</li>
								</ul>
						</li>
						<li>
								<a href="/documents/networking/ipam.html">IPAM and Service Discovery</a>
						</li>
						<li>
								<a href="/documents/networking/ipv6.html">IPv6</a>
						</li>
					</ul>
				</li>
				<li>
					<a href="/documents/storage/index.html">Storage</a>
					<ul class="nav">
						<li>
								<a href="/documents/storage/architecture.html">Architecture</a>
						</li>
						<li>
								<a href="/documents/storage/configuration.html">Configuration</a>
						</li>
						<li>
								<a href="/documents/storage/containers.html">Creating a container with a volume</a>
						</li>
						<li>
								<a href="/documents/storage/volcli.html">Volcli Reference</a>
						</li>
					</ul>
				</li>
				<li>
					<a href="/documents/cluster/index.html">Cluster</a>
					<ul class="nav">
						<li>
								<a href="/documents/cluster/concepts.html">Concepts and Terminology</a>
						</li>
						<li>
								<a href="/documents/cluster/node-lifecycle.html">Node Lifecycle Management</a>
						</li>
					</ul>
				</li>
			</ul>
			<hr><p>
			<h3>Tutorials and Talks</h3>
				<ul class="nav docs-sidenav">
					<li>
						<a href="/documents/tutorials/index.html">Tutorials</a>
						<ul class="nav">
							<li>
									<a href="/documents/tutorials/container-101.html">Container Networking Tutorial</a>
							</li>
							<li>
									<a href="/documents/tutorials/contiv-compose.html">Policies with Networking</a>
							</li>
						</ul>
					</li>
					<li>
						<a href="/documents/demos/index.html">Demonstration Videos</a>
					</li>
					<li>
						<a href="/documents/talks/index.html">Community Talks</a>
					</li>
				</ul>
			<hr><p>
			<h3>Examples</h3>
				<ul class="nav docs-sidenav">
					<ul class="subnav">
						<li>
							<a href="/documents/samples/index.html">Docker Compose Examples</a>
						</li>
						<li>
							<a href="/documents/samples/index.html">Kubernetes Examples</a>
						</li>
						<li>
							<a href="/documents/samples/mcast.html">Multicast Examples</a>
						</li>
						<li>
							<a href="/documents/samples/nomad.html">Nomad Examples</a>
						</li>
					</ul>
				</ul>
			<hr><p>
			<h3>Reference</h3>
				<ul class="nav docs-sidenav">
					<ul class="subnav">
						<li><a href="https://godoc.org/github.com/contiv/contivmodel/client" target="_blank">Network API Reference</a><br></li>
						<li><a href="https://godoc.org/github.com/contiv/volplugin/config" target="_blank">Storage API Reference</a><br></li>
						<li><a href="https://godoc.org/github.com/contiv/cluster" target="_blank">Cluster API Reference</a><br></li>
					</ul>
				</ul>
			<hr><p>
		</div>

		</div>  
		<div id="main-content" class="col-sm-9 col-md-9 col-xs-12" role="main">
			<div class="bs-docs-section">
				
	<h2>Contiv Cluster Installation on Baremetal Server or VM</h2>
<p>This document goes through the steps to install Contiv Cluster to a baremetal server</p>
<p><strong>Note:</strong>
- Unless explicitly mentioned all the steps below are done by logging into the same host. It is referred as <em>control host</em> below.
- Validation of these steps have been done on RHEL 7.2 and CentOS 7.2. More OS variations will be added in future.</p>

<h3>pre-requisites</h3>

<ul>
<li>ansible 2.0 or higher is installed.
</li>
<li>git is installed.
</li>
<li>a management user has been created. Let&#39;s call that user <strong>cluster-admin</strong> from now on.

<ul>
<li>note that <a name="cluster_admin"/><a href="#cluster_admin"><code>cluster-admin</code></a> can be an existing user.
</li>
<li>this user needs to have <strong>passwordless sudo</strong> access configured. You can use <a name="visudo"/><a href="#visudo"><code>visudo</code></a> tool for this.
</li>
</ul>
</li>
<li>a ssh key has been generated for <a name="cluster_admin"/><a href="#cluster_admin"><code>cluster-admin</code></a>. You can use <code>ssh-keygen</code> tool for this.
</li>
<li>the public key for <a name="cluster_admin"/><a href="#cluster_admin"><code>cluster-admin</code></a> user is added to all the hosts(including the control host) in your setup. You can use <code>ssh-copy-id cluster-admin@&lt;hostname&gt;</code> for this, where <code>&lt;hostname&gt;</code> is name of the host in your setup where <code>cluster-admin</code> is being added as authorized user.
</li>
</ul>

<h3>1. Download and install Cluster Manager</h3>
<pre class="highlight plaintext"><code># Login as `cluster-admin` user before running following commands
git clone https://github.com/contiv/ansible.git
cd ansible

# Create an inventory file
echo [cluster-control] &gt; /tmp/hosts
echo node1 ansible_host=127.0.0.1 &gt;&gt; /tmp/hosts

# Install Cluster Manager service
ansible-playbook --key-file=~/.ssh/id_rsa -i /tmp/hosts -e '{"env": {}, "control_interface": "ifname"}' ./site.yml
</code></pre>
<p><strong>Note</strong>:
- <code>env</code> and <code>control_interface</code> need to be specified.
- <code>env</code> is used to specify the environment for running ansible tasks like http-proxy. If there is no special environment to be setup then it needs to be set to an empty dictionary as shown in the example above.
- <code>control_interface</code> is the netdevice that will carry serf traffic on this node.</p>

<h3>2. Configure the cluster manager service</h3>
<p>Edit the cluster manager configuration file that is created at <code>/etc/default/clusterm/clusterm.conf</code> to setup the user and playbook-location information. A sample is shown below. <code>playbook-location</code> needs to be set as the path of ansible directory we cloned in previous step. <code>user</code> needs to be set as name of <code>cluster-admin</code> user and <code>priv_key_file</code> is the location of the <code>id_rsa</code> file of <code>cluster-admin</code> user.</p>
<pre class="highlight plaintext"><code># cat /etc/default/clusterm/clusterm.conf
{
    "ansible": {
        "playbook_location": "/home/cluster-admin/ansible/",
        "user": "cluster-admin",
        "priv_key_file": "/home/cluster-admin/.ssh/id_rsa"
    }
}
</code></pre>
<p>After the changes look good, restart cluster manager</p>
<pre class="highlight plaintext"><code>sudo systemctl restart clusterm
</code></pre>

<h2>Node Life-Cycle management</h2>

<h3>login to the first node to manage the cluster</h3>
<p><strong>Note:</strong> The first node is slightly special in a way that it is booted up with two additional services viz. <code>collins</code> and <code>clusterm</code>. <code>collins</code> is used as the node lifecycle management and event logging service. <code>clusterm</code> is the cluster manager daemon. <code>clusterctl</code> utility is provided to exercise cluster manager provided REST endpoint.</p>

<h3>Provision additional nodes for the cluster formation through discovery</h3>
<pre class="highlight plaintext"><code>clusterctl discover &lt;host-ip(s)&gt;
</code></pre>
<p>Cluster Manager uses Serf as a discovery service for node health monitoring and for cluster bootstrapping. Use <code>discover</code> command to include additional nodes in the discovery service. The <code>&lt;host-ip&gt;</code> should be an IP address from a management network only used by infra services such as serf, etcd, swarm, etc..</p>
<p><strong>Note</strong>:</p>
<pre class="highlight plaintext"><code>clusterctl discover 192.168.2.11 192.168.2.12 --extra-vars='{"env" : {}, "control_interface": "eth1" }'
</code></pre>

<ul>
<li>The command above will provision the other two vms (viz. cluster-node2 and cluster-node3) in the vagrant setup for serf based discovery. Once it is run, the discovered hosts will appear in <a name="clusterctl_nodes_get"/><a href="#clusterctl_nodes_get"><code>clusterctl nodes get</code></a> output in a few minutes.
</li>
<li>the <code>clusterctl discover</code> command expects <code>env</code> and <code>control_interface</code> ansible variables to be specified. This can be achieved by using the <code>--extra-vars</code> flag as shown above or by setting them at <a href="#setget-global-variables">global level</a>, if applicable. For more information on other available variables, also checkout <a href="ansible_vars.md#serf-based-discovery">discovery section of ansible vars</a>
</li>
</ul>

<h3>Get list of discovered nodes</h3>
<pre class="highlight plaintext"><code>clusterctl nodes get
</code></pre>
<p>And info for a single node can be fetched by using <code>clusterctl node get &lt;node-name&gt;</code>.</p>

<h3>Set/Get global variables</h3>
<pre class="highlight plaintext"><code>clusterctl global set --extra-vars=&lt;vars&gt;
</code></pre>
<p>A common set of variables (like environment, scheduler-provider and so on) can be set just once using the <code>--extra-vars</code> flag with <code>clusterctl global set</code> command.</p>
<p><strong>Note</strong>:
- The variables need to be passed as a quoted JSON string using the <code>--extra-vars</code> flag.</p>
<pre class="highlight plaintext"><code>clusterctl global set --extra-vars='{"env" : {"http_proxy": "my.proxy.url"}, "scheduler_provider": "ucp-swarm"}'
</code></pre>

<ul>
<li>The variables set at global level are merged with the variables specified at the node level, with the latter taking precedence in case of an overlap/conflict.
</li>
<li>The list of useful variables is provided at the end of this document at here.
</li>
</ul>

<h3>Commission a node</h3>
<pre class="highlight plaintext"><code>clusterctl node commission &lt;node-name&gt; --host-group=&lt;service-master|service-worker&gt;
</code></pre>
<p>Commissioning a node involves pushing the configuration and starting infra services on that node using <code>ansible</code> based configuration management. The services that are configured depend on the mandatory parameter <code>--host-group</code>. Checkout the <code>service-master</code> and <code>service-worker</code> host-groups in <a href="../vendor/ansible/site.yml">ansible/site.yml</a> to learn more about the services that are configured. To quickly check if commissioning a node worked, you can run <code>etcdctl member list</code> on the node. It shall list all the commissioned members in the list.</p>
<p><strong>Note</strong>:
- certain ansible variables need to be set for provisioning a node. The list of mandatory and other useful variables is provided in <a href="./ansible_vars.md">ansible_vars.md</a>. The variables need to be passed as a quoted JSON string in node commission command using the <code>--extra-vars</code> flag.</p>
<pre class="highlight plaintext"><code>clusterctl node commission node1 --extra-vars='{"env" : {}, "control_interface": "eth1", "netplugin_if": "eth2" }' --host-group "service-master"
</code></pre>

<ul>
<li>a common set of variables (like environment) can be set just once as <a href="#setget-global-variables">global variables</a>. This eliminates the need to specify the common variables for every commission command.
</li>
</ul>

<h3>Decommission a node</h3>
<pre class="highlight plaintext"><code>clusterctl node decommission &lt;node-name&gt;
</code></pre>
<p>Decommissioning a node involves stopping and cleaning the configuration for infra services on that node using <code>ansible</code> based configuration management.</p>

<h3>Perform an upgrade</h3>
<pre class="highlight plaintext"><code>clusterctl node maintain &lt;node-name&gt;
</code></pre>
<p>Upgrading a node involves upgrading the configuration for infra services on that node using <code>ansible</code> based configuration management.</p>

<h3>Get provisioning job status</h3>
<pre class="highlight plaintext"><code>clusterctl job get &lt;active|last&gt;
</code></pre>
<p>Common cluster management workflows like commission, decommission and so on involve running an ansible playbook. Each such run per workflow is referred to as a job. You can see the status of an ongoing (active) or last run job using this command.</p>

<h3>Managing multiple nodes</h3>
<pre class="highlight plaintext"><code>clusterctl nodes commission &lt;space separated node-name(s)&gt;
clusterctl nodes decommission &lt;space separated node-name(s)&gt;
clusterctl nodes maintain &lt;space separated node-name(s)&gt;
</code></pre>
<p>The worflow to commission, decommission or upgrade all or a subset of nodes can be performed by using <code>clusterctl nodes</code> subcommands. Please refer the documentation of individual commands above for details.</p>

<h3>Ansible variables used during provisioning</h3>
<p>This file lists the ansible variables that can be passed at the time of commissioning a node or at a global level as described in <a href="./README.md#setget-global-variables">README.md</a>. The ansible variables can also be passed at the time of setting up a node for discovery as described in <a href="./baremetal.md#3-provision-rest-of-the-nodes-for-discovery-from-the-control-host">baremetal.md</a>. The variables specified at global level are merged with variables specified for a node level operation, with latter taking precedence over the former in case of a overlap/conflict.</p>
<p>Setting the variable at a global level that has same value across all nodes in a cluster, can substantially reduce the amount of variables that need to specified at every node level operation and is a recommended way to set the variables when possible.</p>
<p>The rest of this document is split into two sections viz. <a href="#"><em>Mandatory variables</em></a> and <a href="#"><em>Commonly used variables</em></a>. <em>Mandatory variables</em> lists the variables that must be set before a node can be configured. <em>Commonly used variables</em> lists the variables that we would use to affect the default ansible behavior like deploying a specific scheduler stack or a specific networking mode.</p>
<p><em>Commonly used variables</em> are further organized into following service specific sub-sections:
- <a href="#serf-based-discovery">Serf based Discovery</a>
- <a href="#scheduler-stack">Scheduler stack</a>
- <a href="#contiv-networking">Contiv Networking</a>
- <a href="#contiv-storage">Contiv Storage</a></p>
<p>There are several variables that are made available to provide a good level of programmability in the ansible plays and the reader is encouraged to look at the plays in <a href="../vendor/ansible">vendor/ansible</a></p>

<h3>Mandatory variables</h3>

<ul>
<li><strong>env</strong> is used to set the environment variables that need to be available to ansible tasks. A common usecase of this variable is to set the http-proxy info.
</li>
<li><strong>env</strong> is specified as a JSON dictionary.
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"env"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nt">"var1"</span><span class="p">:</span><span class="w"> </span><span class="s2">"val1"</span><span class="p">,</span><span class="w"> </span><span class="nt">"http_proxy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"http://my.proxy.url"</span><span class="p">,</span><span class="w"> </span><span class="nt">"https_proxy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"http://my.proxy.url"</span><span class="w"> </span><span class="p">}}</span><span class="w">
</span></code></pre>

<ul>
<li>It should be set to empty dictionary if no environment variables needs to be set.
```
{&quot;env&quot;: {}}
</li>
</ul>
<pre class="highlight plaintext"><code>- **control_interface** identifies the netdevice on the node that will carry the traffic generated by infrastructure applications like etcd, ceph and so on.
- **control_interface** is specified as a JSON string

</code></pre>
<p>{&quot;control_interface&quot;: &quot;eth1&quot;}
```
- <strong>netplugin_if</strong> identifies the netdevice on the node that will carry the data traffic generated by the containers networked using contiv data plane.
- <strong>netplugin_if</strong> is specified as a JSON string</p>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"netplugin_if"</span><span class="p">:</span><span class="w"> </span><span class="s2">"eth2"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>service_vip</strong> identifies an available static IP address that can be used as a virtual ip to provide reachability for contiv services.
</li>
<li><strong>service_vip</strong> is specified as a JSON string
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"service_vip"</span><span class="p">:</span><span class="w"> </span><span class="s2">"192.168.2.252"</span><span class="p">}</span><span class="w">
</span></code></pre>

<h3>Optional/Commonly used variables</h3>

<h4>Serf based Discovery</h4>

<ul>
<li><strong>serf_cluster_name</strong> identifies the name of the cluster that serf uses to discover other peer nodes. You may use this if there are multiple clusters in the same subnet of <a name="control_interface"/><a href="#control_interface"><code>control_interface</code></a> and you would like serf to only discover the nodes in a specific cluster.
</li>
<li><strong>serf_cluster_name</strong> is specified as a JSON string
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"serf_cluster_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"cluster-prod-eng"</span><span class="p">}</span><span class="w">
</span></code></pre>

<h4>Scheduler stack</h4>

<ul>
<li><strong>scheduler_provider</strong> identifies the scheduler stack to use. We support two stacks viz. <a name="native_swarm"/><a href="#native_swarm"><code>native-swarm</code></a> and <code>ucp-swarm</code>. The first brings-up a swarm cluster using the stock swarm image from dockerhub. The seconds brings-up a ucp cluster which bundles swarm in it.
</li>
<li><strong>scheduler_provider</strong> is specified as a JSON string
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"scheduler_provider"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ucp-swarm"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>ucp_bootstrap_node_name</strong> identifies the name (as seen in <a name="clusterctl_nodes_get"/><a href="#clusterctl_nodes_get"><code>clusterctl nodes get</code></a> command) of the node to bootstrap ucp with. This is the first node that is commissioned in the cluster. This is mandatory when <strong>scheduler_provider</strong> was set to <code>ucp-swarm</code>
</li>
<li><strong>ucp_bootstrap_node_name</strong> is specified as a JSON string
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"ucp_bootstrap_node_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"cluster-node1-0"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>ucp_license_file</strong> identifies the path to UCP license file on the host where ansible is run. This can be used to pass the UCP license at the time of configuring UCP cluster.
</li>
<li><strong>ucp_license_file</strong> is specified as a JSON string
<code>
{&quot;ucp_license_file&quot;: &quot;/path/to/ucp/licence&quot;}
</code>
</li>
</ul>

<h4>Contiv Networking</h4>

<ul>
<li><strong>contiv_network_mode</strong> identifies the mode of operation for netplugin. Netplugin supports two modes viz. <a name="aci"/><a href="#aci"><code>aci</code></a> and <code>standalone</code>. The first is used to bring-up netplugin in a Cisco APIC managed fabric deployment, while the second mode can be used when deploying netplugin with standalone Layer2/Layer3 switches.
</li>
<li><strong>contiv_network_mode</strong> is specified as a JSON string
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"contiv_network_mode"</span><span class="p">:</span><span class="w"> </span><span class="s2">"aci"</span><span class="p">}</span><span class="w">
</span></code></pre>
<p><strong>Following are the relevant variables when <code>contiv_network_mode</code> is set to <code>aci</code></strong>
- <strong>apic_url</strong> specifies the url for APIC. This is a mandatory variable in aci mode.
- <strong>apic_url</strong> is specified as a JSON string</p>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_url"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://&lt;apic-server-url&gt;:443"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>apic_username</strong> specifies the username for APIC. This is a mandatory variable in aci mode.
</li>
<li><strong>apic_username</strong> is specified as a JSON string
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_username"</span><span class="p">:</span><span class="w"> </span><span class="s2">"my-user"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>apic_password</strong> specifies the password for APIC. This is a mandatory variable in aci mode.
</li>
<li><strong>apic_password</strong> is specified as a JSON string
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_password"</span><span class="p">:</span><span class="w"> </span><span class="s2">"my-password"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>apic_leaf_nodes</strong> specifies full path of the leaf nodes connected managed by APIC. This is a mandatory variable in aci mode.
</li>
<li><strong>apic_leaf_nodes</strong> is specified as a JSON string
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_leaf_nodes"</span><span class="p">:</span><span class="w"> </span><span class="s2">"topology/pod-1/node-101,topology/pod-1/node-102"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>apic_phys_domain</strong> specifies the name of the physical domain name created in APIC.
</li>
<li><strong>apic_phys_domain</strong> is specified as a JSON string
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_phys_domain"</span><span class="p">:</span><span class="w"> </span><span class="s2">"allVlans"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>apic_epg_bridge_domain</strong> can be optionally used to provide a pre-created bridge domain. The bridge domain should have  already been created under tenant <a name="common"/><a href="#common"><code>common</code></a>.
</li>
<li><strong>apic_epg_bridge_domain</strong> is specified as a JSON string
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_epg_bridge_domain"</span><span class="p">:</span><span class="w"> </span><span class="s2">"my-bd"</span><span class="p">}</span><span class="w">
</span></code></pre>

<ul>
<li><strong>apic_contracts_unrestricted_mode</strong> can be optionally used to allow unrestricted communication between EPGs.
</li>
<li><strong>apic_contracts_unrestricted_mode</strong> is specified as a JSON string
</li>
</ul>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"apic_contracts_unrestricted_mode"</span><span class="p">:</span><span class="w"> </span><span class="s2">"yes"</span><span class="p">}</span><span class="w">
</span></code></pre>
<p><strong>Following are the relevant variables when <code>contiv_network_mode</code> is set to <code>standalone</code></strong>
- <strong>fwd_mode</strong> specifies whether netplugin shall bridge or route the packet. Netplugin supports two forwarding modes viz. <code>bridge</code> and <code>routing</code>.
- <strong>fwd_mode</strong> is specified as a JSON string</p>
<pre class="highlight json"><code><span class="p">{</span><span class="nt">"fwd_mode"</span><span class="p">:</span><span class="w"> </span><span class="s2">"routing"</span><span class="p">}</span><span class="w">
</span></code></pre>

<h4>Contiv Storage</h4>
<p><strong>TBD</strong></p>


			</div>
		</div>
	</div>
</div>