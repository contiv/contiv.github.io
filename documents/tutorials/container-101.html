<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Container Networking Tutorial">

    <link rel="shortcut icon" href="/assets/images/favicon.png">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script>
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
    <title>Container Networking Tutorial - Contiv</title>

    <link href="/assets/stylesheets/application-44d25889.css" rel="stylesheet"/>

    <!--[if lt IE 9]><script src="/assets/javascripts/html5shiv-099439cc.js"></script><script src="/assets/javascripts/respond.min-1eb35b04.js"></script><![endif]-->

    
  </head>

  <body id="page-Container Networking Tutorial" class=" page-Container Networking Tutorial layout-documents page-sub">

  <div class="navbar-jumbotron">
    <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/" style="color:$white"> Contiv </a>
        </div>
        <div class="collapse navbar-collapse" id="navbar">
          <ul class="nav navbar-nav navbar-right main-nav">
            <li><a href="/documents/">Docs</a></li>
            <li><a href="https://github.com/contiv/install/releases" target="_blank">Download</a></li>
             <li><a href="/documents/support/index.html">Support</a></li>
            <li><a href="http://blogs.cisco.com/tag/contiv" target="_blank">Blog</a></li>
            
            <li><a href="https://github.com/contiv" target="_blank"><i class="fa fa-github"></i></a></li>
            <li><a href="https://contiv.herokuapp.com/" target="_blank"><i class="fa fa-slack"></i></a></li>
          </ul>
        </div>
      </div>
    </nav>
  </div>


<div class="sidebar-overlay"></div>


<aside id="sidebar" class="sidebar sidebar-default sidebar-fixed-right" role="navigation">
    
    <div class="sidebar-header header-cover">
        
        <div class="sidebar-image">
            <img src="/assets/images/logo3-a96fcb82.png" width="49px" height="56px">
        </div>
    </div>

    
    <ul class="main nav sidebar-nav">
        <li class="first"><a href="/intro/index.html">Intro</a></li>
        <li class=""><a href="/docs/index.html">Docs</a></li>
    </ul>
    <div class="divider"></div>
    
    <ul class="external nav sidebar-nav">
        <li class=""><a class="v-btn gray sml" href="https://github.com/contiv"><svg id="svg-download" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 14" style="enable-background:new 0 0 14 14;">
  <style>
  </style>
  <path class="" d="M13,0H1C0.5,0,0,0.5,0,1v12c0,0.5,0.5,1,1,1h4.7c0,0,0,0,0-0.1c0-0.2,0-0.6,0-1.1c-1.8,0.4-2.2-0.9-2.2-0.9
    c-0.3-0.8-0.7-1-0.7-1c-0.6-0.4,0-0.4,0-0.4c0.7,0,1,0.7,1,0.7c0.6,1,1.5,0.7,1.9,0.5c0.1-0.4,0.2-0.7,0.4-0.9c-1.5-0.2-3-0.7-3-3.2
    c0-0.7,0.3-1.3,0.7-1.8C3.7,5.8,3.5,5.1,3.9,4.2c0,0,0.6-0.2,1.8,0.7c0.5-0.1,1.1-0.2,1.6-0.2c0.6,0,1.1,0.1,1.6,0.2
    c1.3-0.8,1.8-0.7,1.8-0.7c0.4,0.9,0.1,1.6,0.1,1.7c0.4,0.5,0.7,1,0.7,1.8c0,2.5-1.5,3.1-3,3.2C8.7,11.1,9,11.5,9,12.1
    c0,0.9,0,1.6,0,1.8c0,0,0,0,0,0.1h4c0.5,0,1-0.5,1-1V1C14,0.5,13.5,0,13,0z"/>
</svg>
GitHub</a></li>
    </ul>
</aside>


<div class="container">
	<div class="row">
		<div class="col-md-3 col-sm-3 col-xs-12">
					<div class="docs-sidebar hidden-print affix-top" role="complementary">
			<hr>
			<h3>Release Notes</h3>
			<ul class="nav">
				<li>
					<a href="/documents/releasenotes/beta.html">Beta</a>
				</li>
			</ul>
			<hr>
			<h3>Getting Started</h3>
							<ul class="nav">
							<li>
								<a href="/documents/networking/features.html">Features</a>
						    </li>
						    <li>
								<a href="/documents/networking/concepts.html">Concepts and Terminology</a>
						    </li>
								
								<li>
									<a href="https://github.com/contiv/install/blob/master/README.md">Installation</a>
									</li>
								</ul>
									<hr>
			<h3>Admin Guide</h3>
			<ul class="nav">
				<li>
					<a href="/documents/admin/index.html">Intro</a>
				</li>
				<li>
				    <a href="/documents/admin/createTenant.html">Managing Tenants</a>
				</li>
				<li>
				    <a href="/documents/admin/portinfo.html">Managing Ports</a>
				</li>
				<li>
				    <a href="/documents/admin/manageUsers.html">Managing Users</a>
			    </li>		
			    <li>
				    <a href="/documents/admin/manageAuthorizations.html">Managing Authorizations</a>
			    </li>
			    <li>
				    <a href="/documents/admin/createNodes.html">Managing Nodes</a>
			    </li>
			     <li>
				    <a href="/documents/admin/manageNetworks.html">Managing Networks</a>
			    </li>
			    <li>
				    <a href="/documents/admin/manageLDAP.html">Managing LDAP</a>
			    </li>
			</ul>
				</li>
			</ul>
			<hr>
			<h3>User Guide</h3>
						
					<ul class="nav">
						<li>
								<a href="/documents/networking/policies.html">Policies</a>
						</li>
						<li>
								<a href="/documents/networking/services.html">Service Loadbalancing</a>
						</li>
						<li>
								<a class="hasChildren" href="/documents/networking/physical-networks.html">Physical Networks</a>
								<ul class="nav">
									<li>
											<a href="/documents/networking/bgp.html">L3 Routed Networks</a>
									</li>
									<li>
											<a href="/documents/networking/l2-vlan.html">L2 Bridged Networks</a>
									</li>
									<li>
											<a href="/documents/networking/aci_ug.html">Cisco ACI</a>
									</li>
								</ul>
						</li>
						<li>
								<a href="/documents/networking/ipam.html">IPAM and Service Discovery</a>
						</li>
						<li>
								<a href="/documents/networking/ipv6.html">IPv6</a>
						</li>
					</ul>
				</li>
				
				
			</ul>
			<hr><p>
			<h3>Tutorials and Talks</h3>
				<ul class="nav">
					<li class="active">
						<a href="/documents/tutorials/index.html">Tutorials</a>
					</li>
					<li>
						<a href="/documents/demos/index.html">Demonstration Videos</a>
					</li>
					<li>
						<a href="/documents/talks/index.html">Community Talks</a>
					</li>
				</ul>
			<hr><p>
			<h3>OpenShift</h3>
				<ul class="nav">
						<li>
							<a href="/documents/openshift/index.html">Contiv and OpenShift</a>
						</li>
				</ul>
			<hr><p>
			<h3>Examples</h3>
				<ul class="nav">
						<li>
							<a href="/documents/samples/index.html">Docker Compose Examples</a>
						</li>
						<li>
							<a href="/documents/samples/index.html">Kubernetes Examples</a>
						</li>
						<li>
							<a href="/documents/samples/mcast.html">Multicast Examples</a>
						</li>
				</ul>
			<hr><p>
			<h3>Reference</h3>
				<ul class="nav">
					    <li>
					    <a href="/documents/api/contiv.html" target="_blank">API Reference</a>
					    </li> 
					    <li>
					    <a href="/documents/reference/netctlcli.html" target="_blank">CLI Reference</a>
					    </li> 
						<li><a href="https://godoc.org/github.com/contiv/contivmodel/client" target="_blank">Contiv Model Client Library</a>
						</li>
						
				</ul>
			<hr><p>
		</div>

		</div>  
		<div id="main-content" class="col-sm-9 col-md-9 col-xs-12" role="main">
			<div class="bs-docs-section">
				
	<h2>Containers Tutorial</h2>
<p>Walks through container networking and concepts step by step.</p>

<h3>Prerequisites</h3>

<ol>
<li><a href="https://www.vagrantup.com/downloads.html">Download Vagrant</a>
</li>
<li><a href="https://www.virtualbox.org/wiki/Downloads">Download Virtualbox</a>
</li>
</ol>

<h3>Setup</h3>

<h4>Step 1: Get the Vagrantfile</h4>
<p>Copy the <a href="https://raw.githubusercontent.com/jainvipin/tutorial/master/Vagrantfile">contents</a> into a file called Vagrantfile. It could be done using curl as follows</p>
<pre class="highlight plaintext"><code>$ curl https://raw.githubusercontent.com/jainvipin/tutorial/master/Vagrantfile -o Vagrantfile
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  6731  100  6731    0     0   8424      0 --:--:-- --:--:-- --:--:--  8424
</code></pre>
<p>Note: On Windows the file may be downloaded with <code>.txt</code> extension. Rename the file
if needed to remove the extension. Some steps are
documented <a href="http://www.mediacollege.com/microsoft/windows/extension-change.html">here</a>.</p>

<h4>Step 2: Create resolv.conf file in the current directory, where you have copied the Vagrantfile.</h4>
<p>This is used by the VMs to access outside network for downloading docker images in
rest of tutorial. The next steps will fail if this is not specified correctly.
On <code>Mac</code> or <code>Linux</code> based systems you may copy <code>/etc/resolv.conf</code> to current directory.</p>
<pre class="highlight plaintext"><code>$ cp /etc/resolv.conf .

$ cat resolv.conf
domain foobar.com
nameserver 171.70.168.183
nameserver 173.36.131.10

$ ls
Vagrantfile resolv.conf
</code></pre>

<h4>Step 3: Start a small two-node cluster</h4>
<pre class="highlight plaintext"><code>$ vagrant up
Bringing machine 'tutorial-node1' up with 'virtualbox' provider...
Bringing machine 'tutorial-node2' up with 'virtualbox' provider...
 &lt; more output here when trying to bring up the two VMs&gt;
==&gt; tutorial-node1: ++ /opt/bin/startSwarm.py -binpath /opt/bin -nodes 192.168.2.10,192.168.2.11
$
</code></pre>

<h4>Step 4: Log into one of the VMs, verify the installation.</h4>
<p><strong>Note</strong>:
- On Windows, you will need a ssh client to be installed like putty, cygwin etc.
- The username/password for the VMs is vagrant/vagrant</p>
<pre class="highlight plaintext"><code>$ vagrant ssh tutorial-node1
vagrant@tutorial-node1:~$ docker info
</code></pre>
<p>The above command shows the node information, version, etc.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ etcdctl cluster-health
</code></pre>
<p><code>etcdctl</code> is a control utility to manipulate etcd, state store used by kubernetes/docker/contiv</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ ifconfig docker0
docker0   Link encap:Ethernet  HWaddr 02:42:fb:53:27:56  
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:fbff:fe53:2756/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:3521 errors:0 dropped:0 overruns:0 frame:0
          TX packets:3512 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:178318 (178.3 KB)  TX bytes:226978 (226.9 KB)

vagrant@tutorial-node1:~$ ifconfig eth1
eth1      Link encap:Ethernet  HWaddr 08:00:27:f7:17:75  
          inet addr:192.168.2.10  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fef7:1775/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:219585 errors:0 dropped:0 overruns:0 frame:0
          TX packets:272864 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:21413163 (21.4 MB)  TX bytes:28556948 (28.5 MB)

vagrant@tutorial-node1:~$ ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 08:00:27:a2:bc:0d  
          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fea2:bc0d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:28103 errors:0 dropped:0 overruns:0 frame:0
          TX packets:13491 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:36213596 (36.2 MB)  TX bytes:858264 (858.2 KB)
</code></pre>
<p>In the above output, you&#39;ll see:
- <code>docker0</code> interface corresponds to the linux bridge and its associated
subnet <code>172.17.0.1/16</code>. This is created by docker daemon automatically, and
is the default network containers would belong to when an override network
is not specified
- <code>eth0</code> in this VM is the management interface, on which we ssh into the VM
- <code>eth1</code> in this VM is the interface that connects to external network (if needed)
- <code>eth2</code> in this VM is the interface that carries vxlan and control (e.g. etcd) traffic</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl version
</code></pre>
<p><code>netctl</code> is a utility to create, update, read and modify contiv objects. It is a CLI wrapper
on top of REST interface.</p>

<h4>How to restart if VM is shut down, restarted or connectivity is lost</h4>
<p>It is okay to restart the tutorial from where you left at the beginning of the chapter.
The recommended way is to cleanup the setup using <code>vagrant -f destroy</code>, and 
then restarting them again using <code>vagrant up</code></p>

<h3>Chapter 1 - Introduction to Container Networking</h3>
<p>There are two main container networking model discussed within the community.</p>

<h4>Docker libnetwork - Container Network Model (CNM)</h4>
<p>CNM (Container Network Model) is Docker&#39;s libnetwork network model for containers
- An endpoint is container&#39;s interface into a network
- A network is collection of arbitrary endpoints
- A container can belong to multiple endpoints (and therefore multiple networks)
- CNM allows for co-existence of multiple drivers, with a network managed by one driver
- Provides Driver APIs for IPAM and Endpoint creation/deletion
- IPAM Driver APIs: Create/Delete Pool, Allocate/Free IP Address
- Network Driver APIs: Network Create/Delete, Endpoint Create/Delete/Join/Leave
- Used by docker engine, docker swarm, and docker compose; and other schedulers
that schedules regular docker containers e.g. Nomad or Mesos docker containerizer</p>

<h4>CoreOS CNI - Container Network Interface (CNI)</h4>
<p>CNI (Container Network Interface) CoreOS&#39;s network model for containers
- Allows container id (uuid) specification for the network interface you create
- Provides Container Create/Delete events
- Provides access to network namespace to the driver to plumb networking
- No separate IPAM Driver: Container Create returns the IAPM information along with other data
- Used by Kubernetes and thus supported by various Kubernetes network plugins, including Contiv</p>
<p>Using Contiv with CNI/Kubernetes can be found <a href="https://github.com/contiv/netplugin/tree/master/mgmtfn/k8splugin">here</a>.
The rest of the tutorial walks through the docker examples, which implements CNM APIs</p>

<h4>Basic container networking</h4>
<p>Let&#39;s examine the networking a container gets upon vanilla run</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker network ls
NETWORK ID          NAME                DRIVER
4e8800b4788f        bridge              bridge              
ce3dd3b5e270        host                host                
0e7681adac20        none                null             

vagrant@tutorial-node1:~$ docker run -itd --name=vanilla-c alpine /bin/sh
Unable to find image 'alpine:latest' locally
latest: Pulling from library/alpine
b66121b7b9c0: Pull complete 
Digest: sha256:9cacb71397b640eca97488cf08582ae4e4068513101088e9f96c9814bfda95e0
Status: Downloaded newer image for alpine:latest
2cf083c0a4de1347d8fea449dada155b7ef1c99f1f0e684767ae73b3bbb6b533

vagrant@tutorial-node1:~$ ifconfig 
</code></pre>
<p>In the <code>ifconfig</code> output, you will see that it would have created a veth <code>virtual 
ethernet interface</code> that could look like <code>veth......</code> towards the end. More 
importantly it is allocated an IP address from default docker bridge <code>docker0</code>, 
likely <code>172.17.0.3</code> in this setup, and can be examined using</p>
<pre class="highlight plaintext"><code>$ docker network inspect bridge
[
    {
        "Name": "bridge",
        "Id": "4e8800b4788fd21f258d3aa2f4858908b482cefe728106ab92083e604bab8728",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "172.17.0.0/16",
                    "Gateway": "172.17.0.1"
                }
            ]
        },
        "Internal": false,
        "Containers": {
            "39d6c7fa38486a22405c0bb68bffbd7831deb94c690b7353dfd34be97b532a33": {
                "Name": "vanilla-c",
                "EndpointID": "a0697a4aebe2d3ff3c4214b60c8bfaacf8f453670445899822a91b65888a8d01",
                "MacAddress": "02:42:ac:11:00:05",
                "IPv4Address": "172.17.0.5/16",
                "IPv6Address": ""
            },
            "5999ee4484bfd5270b0b22858090135447f2a3175d07d52f1aad6d0073726b77": {
                "Name": "defaultdns",
                "EndpointID": "efbc1c091c6f92b8f68c4053c00c3e383f9bce1dc37fea0fb20970d633c3c6fd",
                "MacAddress": "02:42:ac:11:00:02",
                "IPv4Address": "172.17.0.2/16",
                "IPv6Address": ""
            },
            "ddc0a899164e1d23a62d5ac312f1301bd1586180b1e82175421f2af4e783115d": {
                "Name": "swarm-manager",
                "EndpointID": "c9a353d8c1116f86197edcece771c94a197cf4fb8084d6a1243b8a46f2320708",
                "MacAddress": "02:42:ac:11:00:03",
                "IPv4Address": "172.17.0.3/16",
                "IPv6Address": ""
            },
            "fad30ce6a9658a581306bb7bfc6ea71827810294d97fd77a687a7e86739f9c42": {
                "Name": "swarm-agent",
                "EndpointID": "a24b1c59500f60c8a19821076b8a3869141452f7e493339b715ce42cf537c6c3",
                "MacAddress": "02:42:ac:11:00:04",
                "IPv4Address": "172.17.0.4/16",
                "IPv6Address": ""
            }
        },
        "Options": {
            "com.docker.network.bridge.default_bridge": "true",
            "com.docker.network.bridge.enable_icc": "true",
            "com.docker.network.bridge.enable_ip_masquerade": "true",
            "com.docker.network.bridge.host_binding_ipv4": "0.0.0.0",
            "com.docker.network.bridge.name": "docker0",
            "com.docker.network.driver.mtu": "1500"
        },
        "Labels": {}
    }
]

vagrant@tutorial-node1:~$ docker inspect --format '{{.NetworkSettings.IPAddress}}' vanilla-c
172.17.0.5
</code></pre>
<p>The other pair of veth interface is put into the container with the name <code>eth0</code></p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker exec -it vanilla-c /bin/sh
/ # ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:03  
          inet addr:172.17.0.3  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe11:3%32577/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:8 errors:0 dropped:0 overruns:0 frame:0
          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)
/ # exit
</code></pre>
<p>All traffic to/from this container is Port-NATed to the host&#39;s IP (on eth0).
The Port NATing on the host is done using iptables, which can be seen as a
MASQUERADE rule for outbound traffic for <code>172.17.0.0/16</code></p>
<pre class="highlight plaintext"><code>$ vagrant@tutorial-node1:~$ sudo iptables -t nat -L -n
</code></pre>

<h3>Chapter 2: Multi-host networking</h3>
<p>There are many solutions like Contiv such as Calico, Weave, OpenShift, OpenContrail, Nuage,
VMWare, Docker, Kubernetes, OpenStack that provide solutions to multi-host
container networking.</p>
<p>In this section, let&#39;s examine Contiv and Docker overlay solutions.</p>

<h4>Multi-host networking with Contiv</h4>
<p>Let&#39;s use the same example as above to spin up two containers on the two different hosts</p>

<h4>1. Create a multi-host network</h4>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl net create --subnet=10.1.2.0/24 contiv-net
vagrant@tutorial-node1:~$ netctl net ls
Tenant   Network     Nw Type  Encap type  Packet tag  Subnet       Gateway
------   -------     -------  ----------  ----------  -------      ------
default  contiv-net  data     vxlan       0           10.1.2.0/24  

vagrant@tutorial-node1:~$ $ docker network ls
NETWORK ID          NAME                DRIVER
4e8800b4788f        bridge              bridge              
eeb59152fb69        contiv-net          netplugin           
ce3dd3b5e270        host                host                
0e7681adac20        none                null       

vagrant@tutorial-node1:~$ docker network inspect contiv-net
[
    {
        "Name": "contiv-net",
        "Id": "af2ed043730432e383bbe7fc7716cdfee87246f96342a320ef5fa99f8cf60312",
        "Scope": "global",
        "Driver": "netplugin",
        "IPAM": {
            "Driver": "netplugin",
            "Config": [
                {
                    "Subnet": "10.1.2.0/24"
                }
            ]
        },
        "Containers": {
            "ab353464b4e20b0267d6a078e872fd21730242235667724a9147fdf278a03220": {
                "EndpointID": "5d1720e71e3a4c8da6a8ed361480c094aeb6a3cd3adfe0c7b185690bc64ddcd9",
                "MacAddress": "",
                "IPv4Address": "10.1.2.2/24",
                "IPv6Address": ""
            }
        },
        "Options": {
            "encap": "vxlan",
            "pkt-tag": "1",
            "tenant": "default"
        }
    }
]
</code></pre>
<p>You can now run a new container belonging to <code>contiv-net</code> network:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd --name=contiv-c1 --net=contiv-net alpine /bin/sh
46e619b0b418107114e93f9814963d5c351835624e8da54100b0707582c69549
</code></pre>
<p>Let&#39;s ssh into the second node using <code>vagrant ssh tutorial-node2</code>, and spin up a 
new container on it and try to reach another container running on <code>tutorial-node1</code></p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node2:~$ docker run -itd --name=contiv-c2 --net=contiv-net alpine /bin/sh
26b9f22b9790b55cdfc85f1c2779db5d5fc78c18fee1ea088b85ec0883361a72

vagrant@tutorial-node2:~$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                  PORTS               NAMES
26b9f22b9790        alpine              "/bin/sh"           2 seconds ago       Up Less than a second                       contiv-c2

vagrant@tutorial-node2:~$ docker exec -it contiv-c2 /bin/sh

/ # ping contiv-c1
PING contiv-c1 (10.1.2.3): 56 data bytes
64 bytes from 10.1.2.3: seq=0 ttl=64 time=6.596 ms
64 bytes from 10.1.2.3: seq=1 ttl=64 time=9.451 ms
^C
--- contiv-c1 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 6.596/8.023/9.451 ms

/ # exit
</code></pre>
<p>As you will see during the ping that, built in dns resolves the name <code>contiv-c1</code>
to the IP address of <code>contiv-c1</code> container and be able to reach another container
across using a vxlan overlay.</p>

<h4>Docker Overlay multi-host networking</h4>
<p>Docker engine has a built in overlay driver that can be use to connect
containers across multiple nodes. However since vxlan port used by <code>contiv</code>
driver is same as that of <code>overlay</code> driver from Docker, we will use
Docker&#39;s overlay multi-host networking towards the end after we experiment
with <code>contiv</code> because then we can terminate the contiv driver and
let Docker overlay driver use the vxlan port bindings. More about it in
later chapter.</p>

<h3>Chapter 3: Using multiple tenants with arbitrary IPs in the networks</h3>
<p>First, let&#39;s create a new tenant space</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl tenant create blue
INFO[0000] Creating tenant: blue                    

vagrant@tutorial-node1:~$ netctl tenant ls 
Name     
------   
default  
blue     
</code></pre>
<p>After the tenant is created, we can create network within in tenant <code>blue</code> and run containers.
Here we chose the same subnet and network name for it.
the same subnet and same network name, that we used before</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl net create -t blue --subnet=10.1.2.0/24 contiv-net
vagrant@tutorial-node1:~$ netctl net ls -t blue
Tenant  Network     Nw Type  Encap type  Packet tag  Subnet       Gateway
-/-----  -------     -------  ----------  ----------  -------      ------
blue    contiv-net  data     vxlan       0           10.1.2.0/24  
</code></pre>
<p>Next, we can run containers belonging to this tenant</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd --name=contiv-blue-c1 --net=contiv-net/blue alpine /bin/sh
6c7d8c0b14ec6c2c9f52468faf50444e29c4b1fa61753b75c00f033564814515

vagrant@tutorial-node1:~$ docker ps
CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS              PORTS               NAMES
6c7d8c0b14ec        alpine                         "/bin/sh"           8 seconds ago       Up 6 seconds                            contiv-blue-c1
17afcd58b8fc        skynetservices/skydns:latest   "/skydns"           6 minutes ago       Up 6 minutes        53/tcp, 53/udp      bluedns
46e619b0b418        alpine                         "/bin/sh"           11 minutes ago      Up 11 minutes                           contiv-c1
fb822eda9916        alpine                         "/bin/sh"           23 minutes ago      Up 23 minutes                           overlay-c2
0ab717006962        alpine                         "/bin/sh"           41 minutes ago      Up 41 minutes                           overlay-c1
2cf083c0a4de        alpine                         "/bin/sh"           48 minutes ago      Up 48 minutes                           vanilla-c
ab353464b4e2        skynetservices/skydns:latest   "/skydns"           53 minutes ago      Up 53 minutes       53/udp, 53/tcp      defaultdns
</code></pre>
<p>Let us run a couple of more containers in the host <code>tutorial-node2</code> that belong to the tenant <code>blue</code>:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node2:~$ docker run -itd --name=contiv-blue-c2 --net=contiv-net/blue alpine /bin/sh
521abe19d6b5b3557de6ee4654cc504af0c497a64683f737ffb6f8238ddd6454
vagrant@tutorial-node2:~$ docker run -itd --name=contiv-blue-c3 --net=contiv-net/blue alpine /bin/sh
0fd07b44d042f37069f9a2f7c901867e8fd01c0a5d4cb761123e54e510705c60
vagrant@tutorial-node2:~$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
0fd07b44d042        alpine              "/bin/sh"           7 seconds ago       Up 5 seconds                            contiv-blue-c3
521abe19d6b5        alpine              "/bin/sh"           23 seconds ago      Up 21 seconds                           contiv-blue-c2
26b9f22b9790        alpine              "/bin/sh"           13 minutes ago      Up 13 minutes                           contiv-c2

vagrant@tutorial-node2:~$ docker network inspect contiv-net/blue
[
    {
        "Name": "contiv-net/blue",
        "Id": "4b8448967b7908cab6b3788886aaccc2748bbd85251258de3d01f64e5ee7ae68",
        "Scope": "global",
        "Driver": "netplugin",
        "IPAM": {
            "Driver": "netplugin",
            "Options": {
                "network": "contiv-net",
                "tenant": "blue"
            },
            "Config": [
                {
                    "Subnet": "10.1.2.0/24"
                }
            ]
        },
        "Containers": {
            "0fd07b44d042f37069f9a2f7c901867e8fd01c0a5d4cb761123e54e510705c60": {
                "Name": "contiv-blue-c3",
                "EndpointID": "eab5b2243199e97c29e3be310b37994e31d1203af775d80b0755adfabd52531e",
                "MacAddress": "02:02:0a:01:02:04",
                "IPv4Address": "10.1.2.4/24",
                "IPv6Address": ""
            },
            "521abe19d6b5b3557de6ee4654cc504af0c497a64683f737ffb6f8238ddd6454": {
                "Name": "contiv-blue-c2",
                "EndpointID": "916c971aa4c58c6559c4fab241981d350be15855d0527b83afe4a305f92a4bda",
                "MacAddress": "02:02:0a:01:02:03",
                "IPv4Address": "10.1.2.3/24",
                "IPv6Address": ""
            },
            "ep-3c744285620b746317391c3e27c8d745f2488fae6b23dc1ebe8b217563d2a999": {
                "Name": "bluedns",
                "EndpointID": "3c744285620b746317391c3e27c8d745f2488fae6b23dc1ebe8b217563d2a999",
                "MacAddress": "02:02:0a:01:02:01",
                "IPv4Address": "10.1.2.1/24",
                "IPv6Address": ""
            },
                        "ep-6c7d8c0b14ec6c2c9f52468faf50444e29c4b1fa61753b75c00f033564814515": {
                "Name": "contiv-blue-c1",
                "EndpointID": "c974b747f666d03a9eaab36d20e14f80117328a3e720cba7d17fdf40a57eccf4",
                "MacAddress": "02:02:0a:01:02:02",
                "IPv4Address": "10.1.2.2/24",
                "IPv6Address": ""
            }
        },
        "Options": {
            "encap": "vxlan",
            "pkt-tag": "2",
            "tenant": "blue"
        },
                "Labels": {}
    }
]

vagrant@tutorial-node2:~$ docker exec -it contiv-blue-c3 /bin/sh
/ # ping contiv-blue-c1
PING contiv-blue-c1 (10.1.2.2): 56 data bytes
64 bytes from 10.1.2.2: seq=0 ttl=64 time=60.414 ms
^C
--- contiv-blue-c1 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 60.414/60.414/60.414 ms
/ # ping contiv-blue-c2
PING contiv-blue-c2 (10.1.2.3): 56 data bytes
64 bytes from 10.1.2.3: seq=0 ttl=64 time=1.637 ms
^C
--- contiv-blue-c2 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 1.637/1.637/1.637 ms

/ # exit
</code></pre>

<h3>Chapter 4: Connecting containers to external networks</h3>
<p>In this chapter, we explore ways to connect containers to the external networks</p>

<h4>1. External Connectivity using Host NATing</h4>
<p>Docker uses the linux bridge (docker_gwbridge) based PNAT to reach out and port mappings
for others to reach to the container</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker exec -it contiv-c1 /bin/sh
/ # ifconfig -a
eth0      Link encap:Ethernet  HWaddr 02:02:0A:01:02:03  
          inet addr:10.1.2.3  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::2:aff:fe01:203%32627/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1
          RX packets:19 errors:0 dropped:0 overruns:0 frame:0
          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:1534 (1.4 KiB)  TX bytes:886 (886.0 B)

eth1      Link encap:Ethernet  HWaddr 02:42:AC:12:00:04  
          inet addr:172.18.0.4  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe12:4%32627/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:32 errors:0 dropped:0 overruns:0 frame:0
          TX packets:27 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:5584 (5.4 KiB)  TX bytes:3344 (3.2 KiB)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1%32627/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)

/ # ping contiv.com
PING contiv.com (216.239.34.21): 56 data bytes
64 bytes from 216.239.34.21: seq=0 ttl=61 time=35.941 ms
64 bytes from 216.239.34.21: seq=1 ttl=61 time=38.980 ms
^C
--- contiv.com ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 35.941/37.460/38.980 ms

/ # exit
</code></pre>
<p>What you see is that container has two interfaces belonging to it:
- eth0 is reachability into the <code>contiv-net</code> 
- eth1 is reachability for container to the external world and outside
traffic to be able to reach the container <code>contiv-c1</code>. This also relies on the host&#39;s dns
resolv.conf as a default way to resolve non container IP resolution.</p>
<p>Similarly outside traffic can be exposed on specific ports using <code>-p</code> command. Before
we do that, let us confirm that port 9099 is not reachable from the host
<code>tutorial-node1</code>. To install <code>nc</code> netcat utility please run <code>sudo yum -y install nc</code> on node1</p>
<pre class="highlight plaintext"><code># Install nc utility

vagrant@tutorial-node1:~$ sudo yum -y install nc
&lt; some yum install output &gt;
Complete!

vagrant@tutorial-node1:~$ nc -vw 1 localhost 9099
Ncat: Version 6.40 ( http://nmap.org/ncat )
Ncat: Connection to ::1 failed: Connection refused.
Ncat: Trying next address...
Ncat: Connection refused.
</code></pre>
<p>Now we start a container that exposes tcp port 9099 out in the host.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd -p 9099:9099 --name=contiv-exposed --net=contiv-net alpine /bin/sh
</code></pre>
<p>And if we re-run our <code>nc</code> utility, we&#39;ll see that 9099 is reachable.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ nc -vw 1 localhost 9099
Ncat: Version 6.40 ( http://nmap.org/ncat )
Ncat: Connected to ::1:9099.
^C
</code></pre>
<p>This happens because docker as soon as a port is exposed, a NAT rule is installed for
the port to allow rest of the network to access the container on the specified/exposed
port. The nat rules on the host can be seen by:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ sudo iptables -t nat -L -n
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination         
CONTIV-NODEPORT  all  --  0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL
DOCKER     all  --  0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL

Chain INPUT (policy ACCEPT)
target     prot opt source               destination         

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination         
DOCKER     all  --  0.0.0.0/0           !127.0.0.0/8          ADDRTYPE match dst-type LOCAL

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination         
MASQUERADE  all  --  172.18.0.0/16        0.0.0.0/0           
MASQUERADE  all  --  172.17.0.0/16        0.0.0.0/0           
MASQUERADE  all  --  172.20.0.0/16        0.0.0.0/0           
MASQUERADE  tcp  --  172.17.0.3           172.17.0.3           tcp dpt:2375
MASQUERADE  tcp  --  172.18.0.4           172.18.0.4           tcp dpt:9099

Chain CONTIV-NODEPORT (1 references)
target     prot opt source               destination         

Chain DOCKER (2 references)
target     prot opt source               destination         
RETURN     all  --  0.0.0.0/0            0.0.0.0/0           
RETURN     all  --  0.0.0.0/0            0.0.0.0/0           
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:2375 to:172.17.0.3:2375
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:9099 to:172.18.0.4:9099
</code></pre>

<h4>2. Natively connecting to the external networks</h4>
<p>Remote drivers, like Contiv, can provide an easy way to connect to external
layer2 or layer3 networks using BGP or standard L2 access into the network.</p>
<p>Preferably using an BGP hand-off to the leaf/TOR, this can be done as in 
[<a href="https://github.com/contiv/demo/blob/master/net/Bgp.md">https://github.com/contiv/demo/blob/master/net/Bgp.md</a>], which describes how
can you use BGP with Contiv to provide native container connectivity and 
reachability to rest of the network. However for this tutorial, since we don&#39;t
have a real or simulated BGP router, we&#39;ll use some very simple native L2
connectivity to describe the power of native connectivity. This is done 
using vlan network, for example</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl net create -p 112 -e vlan -s 10.1.3.0/24 contiv-vlan
vagrant@tutorial-node1:~$ netctl net ls
Tenant   Network         Nw Type  Encap type  Packet tag  Subnet       Gateway
------   -------         -------  ----------  ----------  -------      ------
default  contiv-net      data     vxlan       0           10.1.2.0/24  
default  contiv-vlan     data     vlan        112         10.1.3.0/24  
</code></pre>
<p>The allocated vlan can be used to connect any workload in vlan 112 in the network infrastructure.
The interface that connects to the outside network needs to be specified during netplugin
start, for this VM configuration it is set as <code>eth2</code></p>
<p>Let&#39;s run some containers to belong to this network, one on each node. First one on 
<code>tutorial-node1</code></p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd --name=contiv-vlan-c1 --net=contiv-vlan alpine /bin/sh
4bf58874c937e242b4fc2fd8bfd6896a7719fd10475af96e065a83a2e80e9e48
</code></pre>
<p>And another one on <code>tutorial-node2</code></p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node2:~$ docker run -itd --name=contiv-vlan-c2 --net=contiv-vlan alpine /bin/sh
1c463ecad8295b112a7556d1eaf35f1a8152c6b8cfcef1356d40a7b015ae9d02

vagrant@tutorial-node2:~$ docker exec -it contiv-vlan-c2 /bin/sh

/ # ping contiv-vlan-c1
PING contiv-vlan-c1 (10.1.3.4): 56 data bytes
64 bytes from 10.1.3.4: seq=0 ttl=64 time=2.431 ms
64 bytes from 10.1.3.4: seq=1 ttl=64 time=1.080 ms
64 bytes from 10.1.3.4: seq=2 ttl=64 time=1.022 ms
64 bytes from 10.1.3.4: seq=3 ttl=64 time=1.048 ms
64 bytes from 10.1.3.4: seq=4 ttl=64 time=1.119 ms
. . .
</code></pre>
<p>While this is going on <code>tutorial-node2</code>, let&#39;s run tcpdump on eth2 on <code>tutorial-node</code>
and confirm how rx/tx packets look on it:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ sudo tcpdump -e -i eth2 icmp
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth2, link-type EN10MB (Ethernet), capture size 262144 bytes
05:16:51.578066 02:02:0a:01:03:02 (oui Unknown) &gt; 02:02:0a:01:03:04 (oui Unknown), ethertype 802.1Q (0x8100), length 102: vlan 112, p 0, ethertype IPv4, 10.1.3.2 &gt; 10.1.3.4: ICMP echo request, id 2816, seq 294, length 64
05:16:52.588159 02:02:0a:01:03:02 (oui Unknown) &gt; 02:02:0a:01:03:04 (oui Unknown), ethertype 802.1Q (0x8100), length 102: vlan 112, p 0, ethertype IPv4, 10.1.3.2 &gt; 10.1.3.4: ICMP echo request, id 2816, seq 295, length 64
05:16:53.587408 02:02:0a:01:03:02 (oui Unknown) &gt; 02:02:0a:01:03:04 (oui Unknown), ethertype 802.1Q (0x8100), length 102: vlan 112, p 0, ethertype IPv4, 10.1.3.2 &gt; 10.1.3.4: ICMP echo request, id 2816, seq 296, length 64
05:16:54.587550 02:02:0a:01:03:02 (oui Unknown) &gt; 02:02:0a:01:03:04 (oui Unknown), ethertype 802.1Q (0x8100), length 102: vlan 112, p 0, ethertype IPv4, 10.1.3.2 &gt; 10.1.3.4: ICMP echo request, id 2816, seq 297, length 64
ç05:16:55.583786 02:02:0a:01:03:02 (oui Unknown) &gt; 02:02:0a:01:03:04 (oui Unknown), ethertype 802.1Q (0x8100), length 102: vlan 112, p 0, ethertype IPv4, 10.1.3.2 &gt; 10.1.3.4: ICMP echo request, id 2816, seq 298, length 64
^C
5 packets captured
</code></pre>
<p>Note: The vlan shown in tcpdump is same (i.e. <code>112</code>) as what we configured in the VLAN. After verifying this, feel free to stop the ping that is still running on 
<code>contiv-vlan-c2</code> container.</p>

<h3>Chapter 5: Applying policies between containers with Contiv</h3>
<p>Contiv provide a way to apply isolation policies between containers groups.
For this, we create a simple policy called db-policy, and add some rules to which ports are allowed.</p>
<p>Let&#39;s start with <code>tutorial-node1</code> and create the contiv-net if you start this chapter
afresh i.e. after <code>vagrant up</code></p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl net create --subnet=10.1.2.0/24 contiv-net
</code></pre>
<p>Next we create a policy called <code>db-policy</code> to be applied to all db containers.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl policy create db-policy
INFO[0000] Creating policy default:db-policy
vagrant@tutorial-node1:~$ netctl policy ls
Tenant   Policy
------   ------
default  db-policy
</code></pre>
<p>Next we create some rules that are part of the policy:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl policy rule-add db-policy 1 -direction=in -protocol=tcp -action=deny
vagrant@tutorial-node1:~$ netctl policy rule-add db-policy 2 -direction=in -protocol=tcp -port=8888 -action=allow -priority=10
vagrant@tutorial-node1:~$ netctl policy rule-ls db-policy
Incoming Rules:
Rule  Priority  From EndpointGroup  From Network  From IpAddress  Protocol  Port  Action
----  --------  ------------------  ------------  ---------       --------  ----  ------
1     1                                                           tcp       0     deny
2     10                                                          tcp       8888  allow
Outgoing Rules:
Rule  Priority  To EndpointGroup  To Network  To IpAddress  Protocol  Port  Action
----  --------  ----------------  ----------  ---------     --------  ----  ------
</code></pre>
<p>Finally, we associate the policy with a group (a group is an arbitrary collection of 
containers) and run some containers that belong to <code>db</code> group:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl group create contiv-net db-group -policy=db-policy
[vagrant@tutorial-node1 ~]$ netctl group ls
Tenant   Group     Network     Policies   Network profile
------   -----     -------     --------   ---------------
default  db-group  contiv-net  db-policy  

vagrant@tutorial-node1:~$ docker run -itd --name=contiv-db --net=db-group alpine /bin/sh
27eedc376ef43e5a5f3f4ede01635d447b1a0c313cca4c2a640ba4d5dea3573a
</code></pre>
<p>Now let&#39;s verify this policy; in order to do so, we&#39;ll start <code>netcat</code> utility to listen an
arbitrary port within a container that belongs to <code>db</code> group. Then, from another container we
would scan a range of ports and confirm that only the one permitted i.e. <code>port 8888</code> by 
<code>db</code> policy is accessible towards db container</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' contiv-db
10.1.2.5

vagrant@tutorial-node1:~$ docker exec -it contiv-db /bin/sh
/ # nc -l 8888
&lt;awaiting connection&gt;
</code></pre>
<p>Switch over to <code>tutorial-node2</code> window and run a web container and verify the policy. In doing so
please make sure that you enter the IP address of the node discovered above, which in our case was <code>10.1.2.5</code></p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node2:~$ docker run -itd --name=contiv-web --net=contiv-net alpine /bin/sh
54108c756699071d527a567f9b5d266284aaf5299b888d75cd19ba1a40a1135a
vagrant@tutorial-node2:~$ docker exec -it contiv-web /bin/sh

/ # nc -nzvw 1 10.1.2.5 8890
nc: 10.1.2.7 (10.1.2.5:8890): Operation timed out
/ # nc -nzvw 1 10.1.2.5 8889
nc: 10.1.2.7 (10.1.2.5:8889): Operation timed out
/ # nc -nzvw 1 10.1.2.5 8888
/ #
</code></pre>
<p>Note: The last scan on port <code>8888</code> using <code>nc -nzvw 1 10.1.2.5 8888</code> returned
without any <code>Operation timed out</code> message, which means web tier application is able
to reach the database application&#39;s TCP/IP stack.
At this point you can add/delete rules to the policy dynamically.</p>

<h3>Chapter 6: Running containers in a swarm cluster</h3>
<p>We can bring in swarm scheduler by redirecting our requests to an already provision
swarm cluster. To do so, we can set <code>DOCKER_HOST</code> as follows:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ export DOCKER_HOST=tcp://192.168.2.10:2375
</code></pre>
<p>Let&#39;s look at the status of various hosts using <code>docker info</code>:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker info
Containers: 16
Images: 3
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 2
 tutorial-node1: 192.168.2.10:2385
  └ Containers: 10
  └ Reserved CPUs: 0 / 4
  └ Reserved Memory: 0 B / 2.051 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.0.0-040000-generic, operatingsystem=Ubuntu 15.04, storagedriver=overlay
 tutorial-node2: 192.168.2.11:2385
  └ Containers: 6
  └ Reserved CPUs: 0 / 4
  └ Reserved Memory: 0 B / 2.051 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.0.0-040000-generic, operatingsystem=Ubuntu 15.04, storagedriver=overlay
CPUs: 8
Total Memory: 4.103 GiB
Name: tutorial-node1
No Proxy: 192.168.2.10,192.168.2.11,127.0.0.1,localhost,netmaster
</code></pre>
<p>Above command would display the #nodes, #containers in the cluster, available cpu,
memory and other relevant details.</p>
<p>At this point if we run some containers, they will get schedule dynamically across 
the cluster, which is two nodes <code>tutorial-node1</code> and <code>tutorial-node2</code> for us.
To see the node on where the containers get scheduled, we can use <code>docker ps</code> command</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd --name=contiv-cluster-c1 --net=contiv-net alpine /bin/sh
f8fa35c0aa0ecd692a31e3d5249f5c158bd18902926978265acb3b38a9ed1c0d
vagrant@tutorial-node1:~$ docker run -itd --name=contiv-cluster-c2 --net=contiv-net alpine /bin/sh
bfc750736007c307827ae5012755085ca6591f3ac3ac0b707d2befd00e5d1621
</code></pre>
<p>After running two containers, scheduler will schedule these containers using the
scheduling algorithm <code>bin-packing</code> or <code>spread</code>, and if they are not placed on 
different nodes, feel free to start more containers to see the distribution.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker ps
CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS                  PORTS                         NAMES
bfc750736007        alpine                         "/bin/sh"           5 seconds ago       Up Less than a second                                 tutorial-node2/contiv-cluster-c2
f8fa35c0aa0e        alpine                         "/bin/sh"           22 seconds ago      Up 16 seconds                                         tutorial-node2/contiv-cluster-c1
54108c756699        alpine                         "/bin/sh"           14 minutes ago      Up 14 minutes                                         tutorial-node2/contiv-web
27eedc376ef4        alpine                         "/bin/sh"           25 minutes ago      Up 25 minutes                                         tutorial-node1/contiv-db
1c463ecad829        alpine                         "/bin/sh"           39 minutes ago      Up 39 minutes                                         tutorial-node2/contiv-vlan-c2
4bf58874c937        alpine                         "/bin/sh"           40 minutes ago      Up 40 minutes                                         tutorial-node1/contiv-vlan-c1
52bfcc02c362        alpine                         "/bin/sh"           About an hour ago   Up About an hour        192.168.2.10:9099-&gt;9099/tcp   tutorial-node1/contiv-exposed
0fd07b44d042        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                      tutorial-node2/contiv-blue-c3
521abe19d6b5        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                      tutorial-node2/contiv-blue-c2
6c7d8c0b14ec        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                            tutorial-node1/contiv-blue-c1
17afcd58b8fc        skynetservices/skydns:latest   "/skydns"           2 hours ago         Up 2 hours              53/tcp, 53/udp                tutorial-node1/bluedns
26b9f22b9790        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                            tutorial-node2/contiv-c2
46e619b0b418        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                            tutorial-node1/contiv-c1
fb822eda9916        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                            tutorial-node1/overlay-c2
0ab717006962        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                            tutorial-node1/overlay-c1
2cf083c0a4de        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                            tutorial-node1/vanilla-c
ab353464b4e2        skynetservices/skydns:latest   "/skydns"           2 hours ago         Up 2 hours              53/tcp, 53/udp                tutorial-node1/defaultdns
</code></pre>
<p>Let us conclude this section by checking the inter-container connectivity and external 
connectivity for the containers scheduled across multiple hosts.</p>
<pre class="highlight plaintext"><code>agrant@tutorial-node1:~$ docker exec -it contiv-cluster-c1 /bin/sh
/ # 
/ # 
/ # 
/ # ping contiv-cluster-c2
PING contiv-cluster-c2 (10.1.2.7): 56 data bytes
64 bytes from 10.1.2.7: seq=0 ttl=64 time=8.440 ms
64 bytes from 10.1.2.7: seq=1 ttl=64 time=1.479 ms
^C
--- contiv-cluster-c2 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 1.479/4.959/8.440 ms
/ # ping contiv.com
PING contiv.com (216.239.36.21): 56 data bytes
64 bytes from 216.239.36.21: seq=0 ttl=61 time=43.537 ms
64 bytes from 216.239.36.21: seq=1 ttl=61 time=38.867 ms
^C
--- contiv.com ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 38.867/41.202/43.537 ms
</code></pre>

<h4>Chapter 6: Docker Overlay multi-host networking</h4>
<p>As we learned earlier that using the vxlan port conflict can prevent us from using
Docker <code>overlay</code> network. For us to experiment with this, we&#39;d go ahead
and terminate <code>contiv</code> driver first on both nodes: <code>tutorial-node1</code> and
<code>tutorial-node2</code>:</p>
<pre class="highlight plaintext"><code>[vagrant@tutorial-node2 ~]$ ps aux | grep "/opt/bin/netplugin"
vagrant   4227  0.0  0.0 113124  1592 ?        Ss   10:51   0:00 bash -c sudo /opt/bin/netplugin -plugin-mode docker -vlan-if eth2 -cluster-store etcd://localhost:2379 &gt; /tmp/netplugin.log 2&gt;&amp;1
root      4238  0.0  0.1 193436  2784 ?        S    10:51   0:00 sudo /opt/bin/netplugin -plugin-mode docker -vlan-if eth2 -cluster-store etcd://localhost:2379
root      4239  0.5  1.1 576804 21168 ?        Sl   10:51   3:02 /opt/bin/netplugin -plugin-mode docker -vlan-if eth2 -cluster-store etcd://localhost:2379
vagrant  12966  0.0  0.0 112652   980 pts/2    R+   20:03   0:00 grep --color=auto /opt/bin/netplugin
[vagrant@tutorial-node2 ~]$ sudo kill -9 4239
[vagrant@tutorial-node2 ~]$ ps aux | grep "/opt/bin/netplugin"
vagrant  12973  0.0  0.0 112648   980 pts/2    R+   20:03   0:00 grep --color=auto /opt/bin/netplugin
</code></pre>
<p>To try out overlay driver, we switch to <code>tutorial-node1</code> and create an overlay network first.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker network create -d=overlay --subnet=30.1.1.0/24 overlay-net
22f79fe02239d3cbc2c8a4f7147f0e799dc13f3af6e46a69cc3adf8f299a7e56

vagrant@tutorial-node1:~$ docker network inspect overlay-net
[
    {
        "Name": "overlay-net",
        "Id": "22f79fe02239d3cbc2c8a4f7147f0e799dc13f3af6e46a69cc3adf8f299a7e56",
        "Scope": "global",
        "Driver": "overlay",
        "IPAM": {
            "Driver": "default",
            "Config": [
                {
                    "Subnet": "30.1.1.0/24"
                }
            ]
        },
        "Containers": {},
        "Options": {}
    }
]
</code></pre>
<p>Now, we can create few containers that belongs to <code>overlay-net</code>, which can get scheduled
on any of the available nodes by the scheduler. Note that we still have DOCKER_HOST set to point
to the swarm cluster.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd --name=overlay-c1 --net=overlay-net alpine /bin/sh
2cc629162e533f49901df8e1ea47dc1d26076abe65ca34188ce56ba3feb65118

vagrant@tutorial-node1:~$ docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' overlay-c1
30.1.1.2

[vagrant@tutorial-node1 ~]$ docker run -itd --name=overlay-c2 --net=overlay-net alpine /bin/sh
db1fb99d99abc53ad2403f538c8b7919b363bfd7beab0e137b3cef72a097c239

[vagrant@tutorial-node1 ~]$ docker run -itd --name=overlay-c3 --net=overlay-net alpine /bin/sh
29faca64211eb41efff5dc2d717d0c21006b450bbaa4f40819debd76e8a81f0e

[vagrant@tutorial-node1 ~]$ docker ps
CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS              PORTS                         NAMES
29faca64211e        alpine                         "/bin/sh"           4 minutes ago       Up 4 minutes                                      tutorial-node2/overlay-c3
db1fb99d99ab        alpine                         "/bin/sh"           43 minutes ago      Up 43 minutes                                     tutorial-node2/overlay-c2
2cc629162e53        alpine                         "/bin/sh"           51 minutes ago      Up 51 minutes                                     tutorial-node2/overlay-c1
27562283daee        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                  tutorial-node2/contiv-cluster-c2
7ef719672f74        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                  tutorial-node2/contiv-cluster-c1
75f32a4ca91a        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                  tutorial-node2/contiv-web
47da958fa310        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                  tutorial-node1/contiv-db
9dbafcbfabe5        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                  tutorial-node2/contiv-vlan-c2
c011a269cc34        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                  tutorial-node1/contiv-vlan-c1
244181b3c52d        alpine                         "/bin/sh"           About an hour ago   Up About an hour    192.168.2.10:9099-&gt;9099/tcp   tutorial-node1/contiv-exposed
3b8ec5f2a677        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                  tutorial-node2/contiv-blue-c3
d05ea4cae931        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                  tutorial-node2/contiv-blue-c2
2fa04bec0b73        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                  tutorial-node1/contiv-blue-c1
f2e3adbf62e1        skynetservices/skydns:latest   "/skydns"           About an hour ago   Up About an hour    53/tcp, 53/udp                tutorial-node1/bluedns
17fc681fc3f7        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                        tutorial-node1/contiv-c1
39d6c7fa3848        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                        tutorial-node1/vanilla-c
e8afe4a2d051        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                        tutorial-node2/contiv-c2
5999ee4484bf        skynetservices/skydns:latest   "/skydns"           2 days ago          Up 2 days           53/tcp, 53/udp                tutorial-node1/defaultdns

vagrant@tutorial-node2:~$ docker exec -it overlay-c2 /bin/sh
/ # ping overlay-c1
PING overlay-c1 (10.1.1.2): 56 data bytes
64 bytes from 10.1.1.2: seq=0 ttl=64 time=0.066 ms
64 bytes from 10.1.1.2: seq=1 ttl=64 time=0.092 ms
^C
--- overlay-c1 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 0.066/0.079/0.092 ms

/ # exit
</code></pre>
<p>Very similar to contiv-networking, built in dns resolves the name <code>overlay-c1</code>
to the IP address of <code>overlay-c1</code> container and be able to reach another container
across using a vxlan overlay.</p>

<h3>Cleanup: <strong>after all play is done</strong></h3>
<p>To cleanup the setup, after doing all the experiments, exit the VM destroy VMs:</p>
<pre class="highlight plaintext"><code>$ vagrant destroy -f
==&gt; tutorial-node2: Forcing shutdown of VM...
==&gt; tutorial-node2: Destroying VM and associated drives...
==&gt; tutorial-node1: Forcing shutdown of VM...
==&gt; tutorial-node1: Destroying VM and associated drives...
</code></pre>

<h3>References</h3>

<ol>
<li><a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">CNI Specification</a>
</li>
<li><a href="https://github.com/docker/libnetwork/blob/master/docs/design.md">CNM Design</a>
</li>
<li><a href="http://docs.contiv.io">Contiv User Guide</a>
</li>
<li><a href="https://github.com/contiv/netplugin">Contiv Networking Code</a>
</li>
</ol>

<h3>Improvements or Comments</h3>
<p>This tutorial was developed by Contiv engineers. Thank you for trying out this tutorial.
Please file a GitHub issue if you see an issue with the tutorial, or if you prefer
improving some text, feel free to send a pull request.</p>

	<hr>
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <p style="text-align: center"><font color="black">Copyright &copy; 2017 Cisco and/or its affiliates. All rights reserved.</font>
        </p>
      </div>
    </div>
  </div>   

			</div>
		</div>
	</div>
</div>