<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Container Networking Tutorial">

    <link rel="shortcut icon" href="/assets/images/favicon.png">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script>
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
    <title>Container Networking Tutorial - Contiv</title>

    <link href="/assets/stylesheets/application-2b39eabd.css" rel="stylesheet"/>

    <!--[if lt IE 9]><script src="/assets/javascripts/html5shiv-ccddc50c.js"></script><script src="/assets/javascripts/respond.min-1eb35b04.js"></script><![endif]-->

    
  </head>

  <body id="page-Container Networking Tutorial" class=" page-Container Networking Tutorial layout-documents page-sub">

<div id="header" class="navbar-static-top">
  <div class="container">
    <div class="row">
      
        <div class="navbar-header">
          <div class="navbar-brand">
              <a class="logo" href="/">Contiv</a>
            </a>
          </div>
          
        </div>
        <div class="buttons hidden-xs">
          <nav role="navigation">
            <ul class="main-links nav navbar-nav navbar-left">
              <li class="first li-under"><a href="/documents/index.html">Documentation</a></li>
              <li class="li-under"><a href="/articles/index.html">Articles</a></li>
              <li class="li-under"><a href="https://github.com/contiv" target="_blank">
                        GitHub
                        <i class="fa fa-github fa-lg"></i>
                        </a></li>
                    <li class="li-under"><a href="https://contiv-slack.herokuapp.com" target="_blank">
                        Slack
                        <i class="fa fa-slack fa-lg"></i>
                        </a></li>
                    <li><a href="https://twitter.com/projectcontiv" target="_blank">
                        Twitter
                        <i class="fa fa-twitter fa-lg"></i>
                        </a></li>
            </ul>
          </nav>
        </div>
      
    </div>
  </div>
</div>


<div class="sidebar-overlay"></div>


<aside id="sidebar" class="sidebar sidebar-default sidebar-fixed-right" role="navigation">
    
    <div class="sidebar-header header-cover">
        
        <div class="sidebar-image">
            <img src="/assets/images/logo3-a96fcb82.png" width="49px" height="56px">
        </div>
    </div>

    
    <ul class="main nav sidebar-nav">
        <li class="first"><a href="/intro/index.html">Intro</a></li>
        <li class=""><a href="/docs/index.html">Docs</a></li>
    </ul>
    <div class="divider"></div>
    
    <ul class="external nav sidebar-nav">
        <li class=""><a class="v-btn gray sml" href="https://github.com/contiv"><svg id="svg-download" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 14" style="enable-background:new 0 0 14 14;">
  <style>
  </style>
  <path class="" d="M13,0H1C0.5,0,0,0.5,0,1v12c0,0.5,0.5,1,1,1h4.7c0,0,0,0,0-0.1c0-0.2,0-0.6,0-1.1c-1.8,0.4-2.2-0.9-2.2-0.9
    c-0.3-0.8-0.7-1-0.7-1c-0.6-0.4,0-0.4,0-0.4c0.7,0,1,0.7,1,0.7c0.6,1,1.5,0.7,1.9,0.5c0.1-0.4,0.2-0.7,0.4-0.9c-1.5-0.2-3-0.7-3-3.2
    c0-0.7,0.3-1.3,0.7-1.8C3.7,5.8,3.5,5.1,3.9,4.2c0,0,0.6-0.2,1.8,0.7c0.5-0.1,1.1-0.2,1.6-0.2c0.6,0,1.1,0.1,1.6,0.2
    c1.3-0.8,1.8-0.7,1.8-0.7c0.4,0.9,0.1,1.6,0.1,1.7c0.4,0.5,0.7,1,0.7,1.8c0,2.5-1.5,3.1-3,3.2C8.7,11.1,9,11.5,9,12.1
    c0,0.9,0,1.6,0,1.8c0,0,0,0,0,0.1h4c0.5,0,1-0.5,1-1V1C14,0.5,13.5,0,13,0z"/>
</svg>
GitHub</a></li>
    </ul>
</aside>


<div class="container">
	<div class="row">
		<div class="col-md-3 col-sm-3 col-xs-12">
					<div class="docs-sidebar hidden-print affix-top" role="complementary">
			<hr>
			<h3>User Guide</h3>
			<ul class="nav docs-sidenav">
				<li>
					<a href="/documents/gettingStarted/index.html">Getting Started</a>
					<ul class="nav">
						<li>
							<a class="hasChildren" href="/documents/gettingStarted/networking/index.html">Networking</a>
							<ul class="nav">
								<li>
									<a class="hasChildren" href="/documents/gettingStarted/networking/index.html#UsingVagrant">Single Machine</a>
									<ul class="subnav">
										<li>
												<a href="/documents/gettingStarted/networking/swarm.html">Docker Swarm</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/k8s.html">Kubernetes</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/mesos.html">Mesos</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/nomad.html">Nomad</a>
										</li>
									</ul>
								</li>
								<li>
									<a class="hasChildren" href="/documents/gettingStarted/networking/index.html#WithNetworkedServers">Multiple Servers</a>
									<ul class="subnav">
										<li>
												<a href="/documents/gettingStarted/networking/install-swarm.html">Docker Swarm</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/install-k8s.html">Kubernetes</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/bgp.html">BGP</a>
										</li>
										<li>
												<a href="/documents/gettingStarted/networking/aci.html">Cisco ACI</a>
										</li>
									</ul>
								</li>
							</ul>
						</li>
						<li>
							<a href="/documents/gettingStarted/storage/storage.html">Storage </a>
						</li>
						<li>
							<a class="hasChildren" href="/documents/gettingStarted/cluster/index.html">Cluster </a>
							<ul class="subnav">
								<li>
										<a href="/documents/gettingStarted/cluster/vagrant.html">Single Machine</a>
								</li>
								<li>
										<a href="/documents/gettingStarted/cluster/baremetal.html">Multiple Servers</a>
								</li>
							</ul>
						</li>
					</ul>
				</li>
				<li>
					<a href="/documents/networking/index.html">Networking</a>
					<ul class="nav">
						<li>
								<a href="/documents/networking/features.html">Features</a>
						</li>
						<li>
								<a href="/documents/networking/concepts.html">Concepts and Terminology</a>
						</li>
						<li>
								<a href="/documents/networking/policies.html">Policies</a>
						</li>
						<li>
								<a href="/documents/networking/services.html">Service Loadbalancing</a>
						</li>
						<li>
								<a class="hasChildren" href="/documents/networking/physical-networks.html">Physical Networks</a>
								<ul class="nav">
									<li>
											<a href="/documents/networking/bgp.html">L3 Routed Networks</a>
									</li>
									<li>
											<a href="/documents/networking/l2-vlan.html">L2 Bridged Networks</a>
									</li>
									<li>
											<a href="/documents/networking/aci_ug.html">Cisco ACI</a>
									</li>
								</ul>
						</li>
						<li>
								<a href="/documents/networking/ipam.html">IPAM and Service Discovery</a>
						</li>
						<li>
								<a href="/documents/networking/ipv6.html">IPv6</a>
						</li>
					</ul>
				</li>
				<li>
					<a href="/documents/storage/index.html">Storage</a>
					<ul class="nav">
						<li>
								<a href="/documents/storage/architecture.html">Architecture</a>
						</li>
						<li>
								<a href="/documents/storage/configuration.html">Configuration</a>
						</li>
						<li>
								<a href="/documents/storage/containers.html">Creating a container with a volume</a>
						</li>
						<li>
								<a href="/documents/storage/volcli.html">Volcli Reference</a>
						</li>
					</ul>
				</li>
				<li>
					<a href="/documents/cluster/index.html">Cluster</a>
					<ul class="nav">
						<li>
								<a href="/documents/cluster/concepts.html">Concepts and Terminology</a>
						</li>
						<li>
								<a href="/documents/cluster/node-lifecycle.html">Node Lifecycle Management</a>
						</li>
					</ul>
				</li>
			</ul>
			<hr><p>
			<h3>Tutorials and Talks</h3>
				<ul class="nav docs-sidenav">
					<li class="active">
						<a href="/documents/tutorials/index.html">Tutorials</a>
						<ul class="nav">
							<li class="active">
									<a href="/documents/tutorials/container-101.html">Container Networking Tutorial</a>
							</li>
							<li>
									<a href="/documents/tutorials/contiv-compose.html">Policies with Networking</a>
							</li>
						</ul>
					</li>
					<li>
						<a href="/documents/demos/index.html">Demonstration Videos</a>
					</li>
					<li>
						<a href="/documents/talks/index.html">Community Talks</a>
					</li>
				</ul>
			<hr><p>
			<h3>Examples</h3>
				<ul class="nav docs-sidenav">
					<ul class="subnav">
						<li>
							<a href="/documents/samples/index.html">Docker Compose Examples</a>
						</li>
						<li>
							<a href="/documents/samples/index.html">Kubernetes Examples</a>
						</li>
						<li>
							<a href="/documents/samples/mcast.html">Multicast Examples</a>
						</li>
						<li>
							<a href="/documents/samples/nomad.html">Nomad Examples</a>
						</li>
					</ul>
				</ul>
			<hr><p>
			<h3>Reference</h3>
				<ul class="nav docs-sidenav">
					<ul class="subnav">
						<li><a href="https://godoc.org/github.com/contiv/contivmodel/client" target="_blank">Network API Reference</a><br></li>
						<li><a href="https://godoc.org/github.com/contiv/volplugin/config" target="_blank">Storage API Reference</a><br></li>
						<li><a href="https://godoc.org/github.com/contiv/cluster" target="_blank">Cluster API Reference</a><br></li>
					</ul>
				</ul>
			<hr><p>
		</div>

		</div>  
		<div id="main-content" class="col-sm-9 col-md-9 col-xs-12" role="main">
			<div class="bs-docs-section">
				
	<h1>Container Networking Tutorial</h1>
<p>This tutorial is a set of step-by-step container networking examples.
It introduces many of the key concepts in how to network containerized applications.</p>

<h2>Chapter 0: Software Setup</h2>
<p>This tutorial requires a virtual machine (VM) software environment and the
installation of several tools. The tools and environment are easy to set
up using the procedures in this chapter.</p>
<p>You can install the tutorial environment on a Linux, OS X, or Windows computer.</p>

<h3>Prerequisites</h3>
<p>Before you begin, install Vagrant and VirtualBox:</p>

<ul>
<li><a href="https://www.vagrantup.com/downloads.html">Download Vagrant</a>
</li>
<li><a href="https://www.virtualbox.org/wiki/Downloads">Download VirtualBox</a>
</li>
</ul>
<p>If you plan to run the tutorial on Windows, download and install
an ssh client such as <em>putty</em> or <em>Cygwin</em>.</p>

<h3>Setup</h3>
<p>The following steps describe how to set up and start a container cluster.</p>

<h4>Step 1: Download the Vagrant Setup</h4>
<p>Copy the contents of 
<a href="https://raw.githubusercontent.com/jainvipin/tutorial/master/Vagrantfile">this file</a> into a file called Vagrantfile. 
On Linux and OS X machines, you can use <em>curl</em>, as follows:</p>
<pre class="highlight plaintext"><code>$ curl https://raw.githubusercontent.com/jainvipin/tutorial/master/Vagrantfile -o Vagrantfile
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  6731  100  6731    0     0   8424      0 --:--:-- --:--:-- --:--:--  8424
</code></pre>
<p>On Windows the file might download  with <code>.txt</code> extension. If necessary, rename the file
to remove the <code>.txt</code> extension. A way to change the Windows file extension
<a href="http://www.mediacollege.com/microsoft/windows/extension-change.html">is documented here</a>.</p>

<h4>Step 2: Create a <code>resolv.conf</code> File</h4>
<p>In the directory where you copied the Vagrantfile, create a <code>resolv.conf</code> file.
Throughout the rest of this tutorial, this file is used by VMs to access an outside network 
for downloading Docker images.</p>
<pre class="highlight plaintext"><code>$ cp /etc/resolv.conf .

$ cat resolv.conf
domain my_example_domain.com
nameserver 171.70.168.183
nameserver 173.36.131.10
</code></pre>
<p>Verify that your directory contains the <code>Vagrantfile</code> and the <code>resolv.conf</code> file:</p>
<p>$ ls
Vagrantfile resolv.conf
```</p>

<h4>Step 3: Start a Two-node Cluster</h4>
<p>In the directory containing the <code>Vagrantfile</code>, start the tutorial VM cluster
by typing <code>vagrant up</code>:</p>
<pre class="highlight plaintext"><code>$ vagrant up
Bringing machine 'tutorial-node1' up with 'virtualbox' provider...
Bringing machine 'tutorial-node2' up with 'virtualbox' provider...
 &lt; more output here when trying to bring up the two VMs&gt;
==&gt; tutorial-node2: ++ nohup /opt/bin/start-swarm.sh 192.168.2.11 slave
$
</code></pre>

<h4>Step 4: Inspect the VM Services and Utilities</h4>
<p>Next, log into one of the VMs and familiarize yourself with some of the
services and utilities that have been installed for the tutorial. This also confirms
that the services are installed and running properly.</p>
<p>Log into one of the VMs as follows (The username/password for the VMs is vagrant/vagrant):</p>
<pre class="highlight plaintext"><code>$ vagrant ssh tutorial-node1
</code></pre>
<p>The prompt changes, indicating that you are logged into a shell on the VM.</p>
<p>The following command shows some information about Docker services on the node.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker info
</code></pre>
<p>Next, look at  <code>etcdctl</code>, a control utility to manipulate the <em>etcd</em>.
The <em>etcd</em> service is a state store that is used by many of the network 
and container services, including Kubernetes, Docker, and Contiv. Type
the following:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ etcdctl cluster-health
</code></pre>
<p>Now look at the network interfaces on the VM:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ ifconfig docker0
docker0   Link encap:Ethernet  HWaddr 02:42:fb:53:27:56  
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:fbff:fe53:2756/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:3521 errors:0 dropped:0 overruns:0 frame:0
          TX packets:3512 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:178318 (178.3 KB)  TX bytes:226978 (226.9 KB)

vagrant@tutorial-node1:~$ ifconfig eth1
eth1      Link encap:Ethernet  HWaddr 08:00:27:f7:17:75  
          inet addr:192.168.2.10  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fef7:1775/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:219585 errors:0 dropped:0 overruns:0 frame:0
          TX packets:272864 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:21413163 (21.4 MB)  TX bytes:28556948 (28.5 MB)

vagrant@tutorial-node1:~$ ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 08:00:27:a2:bc:0d  
          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fea2:bc0d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:28103 errors:0 dropped:0 overruns:0 frame:0
          TX packets:13491 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:36213596 (36.2 MB)  TX bytes:858264 (858.2 KB)
</code></pre>
<p>Notice the following:</p>

<ul>
<li>The <a name="docker0"/><a href="#docker0"><code>docker0</code></a> interface corresponds to the linux bridge and its associated
subnet <code>172.17.0.1/16</code>. This is created by the Docker daemon automatically, and
is the default network containers belong to when no network is explicitly specified.
</li>
<li><a name="eth0"/><a href="#eth0"><code>eth0</code></a> in this VM is the management interface, which we used to connect to the VM with ssh.
</li>
<li><a name="eth1"/><a href="#eth1"><code>eth1</code></a> in this VM is the interface used to connect to an external network (if needed).
</li>
<li><a name="eth2"/><a href="#eth2"><code>eth2</code></a> in this VM is the interface that carries vxlan and control (for example, etcd) traffic.
</li>
</ul>
<p>Next, check the version of the Contiv Network utility:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl version
</code></pre>
<p><code>netctl</code> is a utility to create, update, read and modify Contiv objects. The <em>netctl</em>
utility is a command-line interface (CLI) wrapper over the Contiv Network REST interface.</p>

<h4>How to Restart</h4>
<p>You can reset the VM cluster if the VM is shut down or restarted, or if connectivity is lost.
Then, restart the tutorial from where you left at the beginning of the chapter.</p>
<p>To reset, type <code>vagrant -f destroy</code> to clean up and stop the cluster,
then restart again by typing <code>vagrant up</code>.</p>

<h2>Chapter 1 - Introduction to Container Networking</h2>
<p>The container community recognized two main container networking models, the
Container Network Model (CNM) and the Container Network Interface (CNI).</p>

<h3>Docker libnetwork - The Container Network Model</h3>
<p>Container Network Model (CNM) is Docker&#39;s <em>libnetwork</em> network model for containers.
It has the following features:</p>

<ul>
<li>An endpoint is container&#39;s interface into a network.
</li>
<li>A network is collection of arbitrary endpoints.
</li>
<li>A container can belong to multiple endpoints (and therefore multiple networks).
</li>
<li>CNM allows for co-existence of multiple drivers, with a network managed by one driver.
</li>
<li>The model provides Driver APIs for IP address management (IPAM) and endpoint creation and deletion.
</li>
<li>The IPAM Driver APIs are: Create/Delete Pool, Allocate/Free IP Address.
</li>
<li>The Network Driver APIs are: Network Create/Delete, Endpoint Create/Delete/Join/Leave.
</li>
<li>Used by Docker Engine, Docker Swarm, and Docker Compose Also used by other schedulers that schedule regular Docker containers, for example  Nomad or Mesos Docker Containerizer.
</li>
</ul>

<h3>CoreOS CNI - Container Network Interface</h3>
<p>Container Network Interface (CNI) is CoreOS&#39;s network model for containers. 
It has the following features:</p>

<ul>
<li>Allows specifying container id (uuid) for which a network interface is created. 
</li>
<li>Provides container Create/Delete events.
</li>
<li>Provides the driver with access to the network namespace to plumb networking.
</li>
<li>Provides no separate IPAM Driver: Container Create returns the IAPM information along with other data.
</li>
<li>Used by Kubernetes and supported by various Kubernet network plugins, including Contiv.
</li>
</ul>
<p><a href="https://github.com/contiv/netplugin/tree/master/mgmtfn/k8splugin">Click here</a> to read more about using Contiv with CNI and Kubernetes.</p>
<p>The examples throughout the rest of this tutorial use Docker, which implements the CNM APIs.</p>

<h4>Basic Container Networking</h4>
<p>Let&#39;s examine the networking supplied with a &quot;plain vanilla&quot; basic container. 
In the Docker VM, type to commands shown:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker network ls
NETWORK ID          NAME                DRIVER
3c5ec5bc780a        none                null                
fb44f53e1e91        host                host                
6a63b892d974        bridge              bridge              

vagrant@tutorial-node1:~$ docker run -itd --name=vanilla-c alpine /bin/sh
Unable to find image 'alpine:latest' locally
latest: Pulling from library/alpine
b66121b7b9c0: Pull complete 
Digest: sha256:9cacb71397b640eca97488cf08582ae4e4068513101088e9f96c9814bfda95e0
Status: Downloaded newer image for alpine:latest
2cf083c0a4de1347d8fea449dada155b7ef1c99f1f0e684767ae73b3bbb6b533

vagrant@tutorial-node1:~$ ifconfig 
</code></pre>
<p>In the <code>ifconfig</code> output, notice the creation of a veth <code>virtual 
ethernet interface</code> that looks something like <code>veth......</code> towards the end. More 
importantly, it is allocated an IP address from the default docker bridge <code>docker0</code>, 
probably <code>172.17.0.3</code> in this setup. Examine the bridge as follows:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker network inspect bridge
[
    {
        "Name": "bridge",
        "Id": "6a63b892d97413275ab32e8792d32498848ad32bfb78d3cfd74a82ce5cbc46c2",
        "Scope": "local",
        "Driver": "bridge",
        "IPAM": {
            "Driver": "default",
            "Config": [
                {
                    "Subnet": "172.17.0.1/16",
                    "Gateway": "172.17.0.1"
                }
            ]
        },
        "Containers": {
            "2cf083c0a4de1347d8fea449dada155b7ef1c99f1f0e684767ae73b3bbb6b533": {
                "EndpointID": "c0cebaaf691c3941fca1dae4a8d2b3a94c511027f15d4c27b40606f7fb937f24",
                "MacAddress": "02:42:ac:11:00:03",
                "IPv4Address": "172.17.0.3/16",
                "IPv6Address": ""
            },
            "ab353464b4e20b0267d6a078e872fd21730242235667724a9147fdf278a03220": {
                "EndpointID": "6259ca5d2267f02d139bbcf55cb15b4ad670edefb5f4308e47da399beb1dc62c",
                "MacAddress": "02:42:ac:11:00:02",
                "IPv4Address": "172.17.0.2/16",
                "IPv6Address": ""
            }
        },
        "Options": {
            "com.docker.network.bridge.default_bridge": "true",
            "com.docker.network.bridge.enable_icc": "true",
            "com.docker.network.bridge.enable_ip_masquerade": "true",
            "com.docker.network.bridge.host_binding_ipv4": "0.0.0.0",
            "com.docker.network.bridge.name": "docker0",
            "com.docker.network.driver.mtu": "1500"
        }
    }
]
</code></pre>
<p>See just the IP address like this:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker inspect --format '{{.NetworkSettings.IPAddress}}' vanilla-c
172.17.0.3
</code></pre>
<p>The other pair of veth interfaces are added to the container with the name <code>eth0</code>:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker exec -it vanilla-c /bin/sh
/ # ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:03  
          inet addr:172.17.0.3  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe11:3%32577/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:8 errors:0 dropped:0 overruns:0 frame:0
          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)
/ # exit
</code></pre>
<p>All traffic to and from this container is port-NATed to the host&#39;s IP (on eth0).
The Port NATing on the host is done using iptables, which can be seen as a
MASQUERADE rule for outbound traffic for <code>172.17.0.0/16</code>:</p>
<pre class="highlight plaintext"><code>$ vagrant@tutorial-node1:~$ sudo iptables -t nat -L -n
</code></pre>

<h2>Chapter 2: Multi-Host Networking</h2>
<p>There are many products that provide solutions for multi-host container networking. 
These include Contiv, Calico, Weave, Openshift, OpenContrail, Nuage, VMWare, Docker, 
Kubernetes, and Openstack. In this section we examine Contiv and Docker overlay solutions.</p>

<h3>Multi-Host Networking with Contiv</h3>
<p>Create two containers on the two different hosts. This requires three steps.</p>
<p>First, do the following to create and examine a new virtual network called <code>contiv-net</code>:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl net create --subnet=10.1.2.0/24 contiv-net
vagrant@tutorial-node1:~$ netctl net ls
Tenant   Network     Nw Type  Encap type  Packet tag  Subnet       Gateway
------   -------     -------  ----------  ----------  -------      ------
default  contiv-net  data     vxlan       0           10.1.2.0/24  

vagrant@tutorial-node1:~$ docker network ls
NETWORK ID          NAME                DRIVER
22f79fe02239        overlay-net         overlay             
af2ed0437304        contiv-net          netplugin           
6a63b892d974        bridge              bridge              
3c5ec5bc780a        none                null                
fb44f53e1e91        host                host                
cf7ccff07b64        docker_gwbridge     bridge              

vagrant@tutorial-node1:~$ docker network inspect contiv-net
[
    {
        "Name": "contiv-net",
        "Id": "af2ed043730432e383bbe7fc7716cdfee87246f96342a320ef5fa99f8cf60312",
        "Scope": "global",
        "Driver": "netplugin",
        "IPAM": {
            "Driver": "netplugin",
            "Config": [
                {
                    "Subnet": "10.1.2.0/24"
                }
            ]
        },
        "Containers": {
            "ab353464b4e20b0267d6a078e872fd21730242235667724a9147fdf278a03220": {
                "EndpointID": "5d1720e71e3a4c8da6a8ed361480c094aeb6a3cd3adfe0c7b185690bc64ddcd9",
                "MacAddress": "",
                "IPv4Address": "10.1.2.2/24",
                "IPv6Address": ""
            }
        },
        "Options": {
            "encap": "vxlan",
            "pkt-tag": "1",
            "tenant": "default"
        }
    }
]
</code></pre>
<p>Second, run a new container belonging to the <code>contiv-net</code> network:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd --name=contiv-c1 --net=contiv-net alpine /bin/sh
46e619b0b418107114e93f9814963d5c351835624e8da54100b0707582c69549
</code></pre>
<p>Finally, use ssh to log into the second node using <code>vagrant ssh tutorial-node2</code>, create a
new container on it, and try to reach another container running on <code>tutorial-node1</code>:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node2:~$ docker run -itd --name=contiv-c2 --net=contiv-net alpine /bin/sh
26b9f22b9790b55cdfc85f1c2779db5d5fc78c18fee1ea088b85ec0883361a72

vagrant@tutorial-node2:~$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                  PORTS               NAMES
26b9f22b9790        alpine              "/bin/sh"           2 seconds ago       Up Less than a second                       contiv-c2

vagrant@tutorial-node2:~$ docker exec -it contiv-c2 /bin/sh

/ # ping contiv-c1
PING contiv-c1 (10.1.2.3): 56 data bytes
64 bytes from 10.1.2.3: seq=0 ttl=64 time=6.596 ms
64 bytes from 10.1.2.3: seq=1 ttl=64 time=9.451 ms
^C
--- contiv-c1 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 6.596/8.023/9.451 ms

/ # exit
</code></pre>
<p>The <code>ping</code> command demonstrates that built-in DNS resolves the name <code>overlay-c1</code>
to the IP address of the <code>overlay-c1</code> container. The container <code>contiv-c2</code> is
ableto reach the container on the other VM across the network using a vxlan overlay.</p>

<h3>Docker Overlay Multi-host Networking</h3>
<p>Docker engine has a built in overlay driver that can be use to connect
containers across multiple nodes.</p>
<p>Create another network as follows:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker network create -d=overlay --subnet=10.1.1.0/24 overlay-net
22f79fe02239d3cbc2c8a4f7147f0e799dc13f3af6e46a69cc3adf8f299a7e56

vagrant@tutorial-node1:~$ docker network inspect overlay-net
[
    {
        "Name": "overlay-net",
        "Id": "22f79fe02239d3cbc2c8a4f7147f0e799dc13f3af6e46a69cc3adf8f299a7e56",
        "Scope": "global",
        "Driver": "overlay",
        "IPAM": {
            "Driver": "default",
            "Config": [
                {
                    "Subnet": "10.1.1.0/24"
                }
            ]
        },
        "Containers": {},
        "Options": {}
    }
]
</code></pre>
<p>Now create a container that belongs to <code>overlay-net</code>:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd --name=overlay-c1 --net=overlay-net alpine /bin/sh
0ab717006962fb2fe936aa3c133dd27d68c347d5f239f473373c151ad4c77b28

vagrant@tutorial-node1:~$ docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' overlay-c1
10.1.1.2
</code></pre>
<p>As before, run another container belonging to the same network within the cluster and
make sure that they can reach each other.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd --name=overlay-c2 --net=overlay-net alpine /bin/sh
0a6f43693361855ad56cda417b9ec63f504de4782ac82f0181fed92d803b4a30
vagrant@tutorial-node1:~$ docker ps
CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS              PORTS               NAMES
fb822eda9916        alpine                         "/bin/sh"           3 minutes ago       Up 3 minutes                            overlay-c2
0ab717006962        alpine                         "/bin/sh"           21 minutes ago      Up 21 minutes                           overlay-c1
2cf083c0a4de        alpine                         "/bin/sh"           28 minutes ago      Up 28 minutes                           vanilla-c
ab353464b4e2        skynetservices/skydns:latest   "/skydns"           33 minutes ago      Up 33 minutes       53/tcp, 53/udp      defaultdns

vagrant@tutorial-node2:~$ docker exec -it overlay-c2 /bin/sh
/ # ping overlay-c1
PING overlay-c1 (10.1.1.2): 56 data bytes
64 bytes from 10.1.1.2: seq=0 ttl=64 time=0.066 ms
64 bytes from 10.1.1.2: seq=1 ttl=64 time=0.092 ms
^C
--- overlay-c1 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 0.066/0.079/0.092 ms

/ # exit
</code></pre>
<p>Similar to the contiv-networking example, built-in DNS resolves the name <code>overlay-c1</code>
to the IP address of the <code>overlay-c1</code> container and one container can reach the other container
across using a vxlan overlay.</p>

<h2>Chapter 3: Multiple Tenants, Arbitrary IPs.</h2>
<p>Now we add multiple tenants with arbitrary IPs in their networks.</p>
<p>First, create a new tenant space:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl tenant create blue
INFO[0000] Creating tenant: blue                    

vagrant@tutorial-node1:~$ netctl tenant ls 
Name     
------   
default  
blue     
</code></pre>
<p>After the tenant is created, you can create a network within tenant <code>blue</code> and run containers.
Do so, using the same subnet and network name as in the previous exercise:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl net create -t blue --subnet=10.1.2.0/24 contiv-net
vagrant@tutorial-node1:~$ netctl net ls -t blue
Tenant  Network     Nw Type  Encap type  Packet tag  Subnet       Gateway
-/-----  -------     -------  ----------  ----------  -------      ------
blue    contiv-net  data     vxlan       0           10.1.2.0/24  
</code></pre>
<p>Next, run containers belonging to this tenant:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd --name=contiv-blue-c1 --net=contiv-net/blue alpine /bin/sh
6c7d8c0b14ec6c2c9f52468faf50444e29c4b1fa61753b75c00f033564814515

vagrant@tutorial-node1:~$ docker ps
CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS              PORTS               NAMES
6c7d8c0b14ec        alpine                         "/bin/sh"           8 seconds ago       Up 6 seconds                            contiv-blue-c1
17afcd58b8fc        skynetservices/skydns:latest   "/skydns"           6 minutes ago       Up 6 minutes        53/tcp, 53/udp      bluedns
46e619b0b418        alpine                         "/bin/sh"           11 minutes ago      Up 11 minutes                           contiv-c1
fb822eda9916        alpine                         "/bin/sh"           23 minutes ago      Up 23 minutes                           overlay-c2
0ab717006962        alpine                         "/bin/sh"           41 minutes ago      Up 41 minutes                           overlay-c1
2cf083c0a4de        alpine                         "/bin/sh"           48 minutes ago      Up 48 minutes                           vanilla-c
ab353464b4e2        skynetservices/skydns:latest   "/skydns"           53 minutes ago      Up 53 minutes       53/udp, 53/tcp      defaultdns
</code></pre>
<p>Run a couple more containers in the host <code>tutorial-node2</code> that belong to the tenatn <code>blue</code>:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node2:~$ docker run -itd --name=contiv-blue-c2 --net=contiv-net/blue alpine /bin/sh
521abe19d6b5b3557de6ee4654cc504af0c497a64683f737ffb6f8238ddd6454
vagrant@tutorial-node2:~$ docker run -itd --name=contiv-blue-c3 --net=contiv-net/blue alpine /bin/sh
0fd07b44d042f37069f9a2f7c901867e8fd01c0a5d4cb761123e54e510705c60
vagrant@tutorial-node2:~$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
0fd07b44d042        alpine              "/bin/sh"           7 seconds ago       Up 5 seconds                            contiv-blue-c3
521abe19d6b5        alpine              "/bin/sh"           23 seconds ago      Up 21 seconds                           contiv-blue-c2
26b9f22b9790        alpine              "/bin/sh"           13 minutes ago      Up 13 minutes                           contiv-c2

vagrant@tutorial-node2:~$ docker network inspect contiv-net/blue
[
    {
        "Name": "contiv-net/blue",
        "Id": "4b8448967b7908cab6b3788886aaccc2748bbd85251258de3d01f64e5ee7ae68",
        "Scope": "global",
        "Driver": "netplugin",
        "IPAM": {
            "Driver": "netplugin",
            "Config": [
                {
                    "Subnet": "10.1.2.0/24"
                }
            ]
        },
        "Containers": {
            "0fd07b44d042f37069f9a2f7c901867e8fd01c0a5d4cb761123e54e510705c60": {
                "EndpointID": "4688209bd46047f1e9ab016fadff7bdf7c012cbfa253ec6a3661742f84ca5feb",
                "MacAddress": "",
                "IPv4Address": "10.1.2.4/24",
                "IPv6Address": ""
            },
            "521abe19d6b5b3557de6ee4654cc504af0c497a64683f737ffb6f8238ddd6454": {
                "EndpointID": "4d9ca7047b8737b78f271a41db82bbf5c3004f297211d831af757f565fc0c691",
                "IPv4Address": "10.1.2.3/24",
                "IPv6Address": ""
            }
        },
        "Options": {
            "encap": "vxlan",
            "pkt-tag": "2",
            "tenant": "blue"
        }
    }
]

vagrant@tutorial-node2:~$ docker exec -it contiv-blue-c3 /bin/sh
/ # ping contiv-blue-c1
PING contiv-blue-c1 (10.1.2.2): 56 data bytes
64 bytes from 10.1.2.2: seq=0 ttl=64 time=60.414 ms
^C
--- contiv-blue-c1 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 60.414/60.414/60.414 ms
/ # ping contiv-blue-c2
PING contiv-blue-c2 (10.1.2.3): 56 data bytes
64 bytes from 10.1.2.3: seq=0 ttl=64 time=1.637 ms
^C
--- contiv-blue-c2 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 1.637/1.637/1.637 ms

/ # exit
</code></pre>
<p>As expected, the containers are reachable to each other.</p>

<h3>Chapter 4: Connecting Containers to External Networks</h3>
<p>In this chapter, we explore ways to connect containers to external networks.</p>

<h4>External Connectivity using Host NATing</h4>
<p>Docker uses the Linux bridge (docker_gwbridge)-based PNAT to reach out. 
It uses port mappings to enable others to reach the container.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker exec -it contiv-c1 /bin/sh
/ # ifconfig -a
eth0      Link encap:Ethernet  HWaddr 02:02:0A:01:02:03  
          inet addr:10.1.2.3  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::2:aff:fe01:203%32627/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1
          RX packets:19 errors:0 dropped:0 overruns:0 frame:0
          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:1534 (1.4 KiB)  TX bytes:886 (886.0 B)

eth1      Link encap:Ethernet  HWaddr 02:42:AC:12:00:04  
          inet addr:172.18.0.4  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe12:4%32627/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:32 errors:0 dropped:0 overruns:0 frame:0
          TX packets:27 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:5584 (5.4 KiB)  TX bytes:3344 (3.2 KiB)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1%32627/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)

/ # ping contiv.com
PING contiv.com (216.239.34.21): 56 data bytes
64 bytes from 216.239.34.21: seq=0 ttl=61 time=35.941 ms
64 bytes from 216.239.34.21: seq=1 ttl=61 time=38.980 ms
^C
--- contiv.com ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 35.941/37.460/38.980 ms

/ # exit
</code></pre>
<p>Notice that the container has two interfaces:</p>

<ul>
<li>eth0 connects into the <a name="contiv_net"/><a href="#contiv_net"><code>contiv-net</code></a> 
</li>
<li>eth1 connects the container to the external world and enables outside traffic to reach the container <a name="contiv_c1"/><a href="#contiv_c1"><code>contiv-c1</code></a>. This connectivity also relies on the host&#39;s DNS <code>resolv.conf</code> for default non-container IP resolution.
</li>
</ul>
<p>Similarly, outside traffic can be exposed on specific ports using <code>-p</code> command.</p>
<p>First,confirm that port 9099 is not reachable from the host <code>tutorial-node1</code>:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ nc -zvw 1 localhost 9099
nc: connect to localhost port 9099 (tcp) failed: Connection refused
nc: connect to localhost port 9099 (tcp) failed: Connection refused
</code></pre>
<p>Now start a container that exposes TCP port 9099 out in the host.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd -p 9099:9099 --name=contiv-exposed --net=contiv-net alpine /bin/sh
</code></pre>
<p>Re-run the <code>nc</code> utility, and note that port 9099 is reachable:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ nc -zvw 1 localhost 9099
Connection to localhost 9099 port [tcp/*] succeeded!
</code></pre>
<p>This happens because in Docker as soon as a port is exposed, a NAT rule is installed that
allows the rest of the network to access the container on the newly exposed
port. Examine the NAT rule like this:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ sudo iptables -t nat -L -n
iptables v1.4.21: can't initialize iptables table `nat': Permission denied (you must be root)
Perhaps iptables or your kernel needs to be upgraded.
vagrant@tutorial-node1:~$ sudo iptables -t nat -L -n
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination         
DOCKER     all  --  0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL

Chain INPUT (policy ACCEPT)
target     prot opt source               destination         

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination         
DOCKER     all  --  0.0.0.0/0           !127.0.0.0/8          ADDRTYPE match dst-type LOCAL

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination         
MASQUERADE  all  --  172.18.0.0/16        0.0.0.0/0           
MASQUERADE  all  --  172.17.0.0/16        0.0.0.0/0           
MASQUERADE  tcp  --  172.18.0.6           172.18.0.6           tcp dpt:9099

Chain DOCKER (2 references)
target     prot opt source               destination         
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:9099 to:172.18.0.6:9099

</code></pre>

<h3>Natively Connecting to External Networks</h3>
<p>Remote drivers like Contiv provide an easy way to connect to external
layer 2 or layer 3 networks using BGP or standard L2 access into the network.</p>
<p>This can be done Using a BGP hand-off to the leaf or top-of-rack (TOR) switch. 
<a href="/install/user_guide/getting_started/networking/bgp.html">Click here</a> for
an example that describes how you can use BGP with Contiv to provide native 
container connectivity and reachability to rest of the network.</p>
<p>For this tutorial, since we don&#39;t have a real or simulated BGP router, 
we&#39;ll use some very simple native L2 connectivity to show the power of native connectivity.</p>
<p>Start by creating a VLAN network:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl net create -p 112 -e vlan -s 10.1.3.0/24 contiv-vlan
vagrant@tutorial-node1:~$ netctl net ls
Tenant   Network         Nw Type  Encap type  Packet tag  Subnet       Gateway
------   -------         -------  ----------  ----------  -------      ------
default  contiv-net      data     vxlan       0           10.1.2.0/24  
default  contiv-vlan     data     vlan        112         10.1.3.0/24  
</code></pre>
<p>The VLAN can be used to connect any workload in VLAN 112 in the network infrstructure.
The interface that connects to the outside network must be specified during netplugin
start; for this VM configuration it is set as <code>eth2</code>.</p>
<p>Run some containers to belong to this network, one on each node. Create the first on
<code>tutorial-node1</code>:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd --name=contiv-vlan-c1 --net=contiv-vlan alpine /bin/sh
4bf58874c937e242b4fc2fd8bfd6896a7719fd10475af96e065a83a2e80e9e48
</code></pre>
<p>And the second on <code>tutorial-node2</code>:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node2:~$ docker run -itd --name=contiv-vlan-c2 --net=contiv-vlan alpine /bin/sh
1c463ecad8295b112a7556d1eaf35f1a8152c6b8cfcef1356d40a7b015ae9d02

vagrant@tutorial-node2:~$ docker exec -it contiv-vlan-c2 /bin/sh

/ # ping contiv-vlan-c1
PING contiv-vlan-c1 (10.1.3.4): 56 data bytes
64 bytes from 10.1.3.4: seq=0 ttl=64 time=2.431 ms
64 bytes from 10.1.3.4: seq=1 ttl=64 time=1.080 ms
64 bytes from 10.1.3.4: seq=2 ttl=64 time=1.022 ms
64 bytes from 10.1.3.4: seq=3 ttl=64 time=1.048 ms
64 bytes from 10.1.3.4: seq=4 ttl=64 time=1.119 ms
. . .
</code></pre>
<p>While the <code>ping</code> runs on <code>tutorial-node2</code>, run <code>tcpdump</code> on eth2 on <code>tutorial-node1</code>
and look at the rx/tx packets:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ sudo tcpdump -e -i eth2 icmp
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth2, link-type EN10MB (Ethernet), capture size 262144 bytes
05:16:51.578066 02:02:0a:01:03:02 (oui Unknown) &gt; 02:02:0a:01:03:04 (oui Unknown), ethertype 802.1Q (0x8100), length 102: vlan 112, p 0, ethertype IPv4, 10.1.3.2 &gt; 10.1.3.4: ICMP echo request, id 2816, seq 294, length 64
05:16:52.588159 02:02:0a:01:03:02 (oui Unknown) &gt; 02:02:0a:01:03:04 (oui Unknown), ethertype 802.1Q (0x8100), length 102: vlan 112, p 0, ethertype IPv4, 10.1.3.2 &gt; 10.1.3.4: ICMP echo request, id 2816, seq 295, length 64
05:16:53.587408 02:02:0a:01:03:02 (oui Unknown) &gt; 02:02:0a:01:03:04 (oui Unknown), ethertype 802.1Q (0x8100), length 102: vlan 112, p 0, ethertype IPv4, 10.1.3.2 &gt; 10.1.3.4: ICMP echo request, id 2816, seq 296, length 64
05:16:54.587550 02:02:0a:01:03:02 (oui Unknown) &gt; 02:02:0a:01:03:04 (oui Unknown), ethertype 802.1Q (0x8100), length 102: vlan 112, p 0, ethertype IPv4, 10.1.3.2 &gt; 10.1.3.4: ICMP echo request, id 2816, seq 297, length 64
ç05:16:55.583786 02:02:0a:01:03:02 (oui Unknown) &gt; 02:02:0a:01:03:04 (oui Unknown), ethertype 802.1Q (0x8100), length 102: vlan 112, p 0, ethertype IPv4, 10.1.3.2 &gt; 10.1.3.4: ICMP echo request, id 2816, seq 298, length 64
^C
5 packets captured
</code></pre>
<p>Note that the VLAN shown in <code>tcpdump</code> is same (<code>112</code>) as you configured in the VLAN. 
After verifying this, stop the ping that is still running on <code>contiv-vlan-c2</code>.</p>

<h2>Chapter 5: Applying Policies Between Containers with Contiv</h2>
<p>Contiv provide a way to apply isolation policies between containers groups.
To demonstrate, create a simple policy called db-policy, and add some rules to 
which ports are allowed.</p>
<p>Start with <code>tutorial-node1</code> and create the <code>contiv-net</code> (if you have restarted
the environment after the previous exercise).</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl net create --subnet=10.1.2.0/24 contiv-net
</code></pre>
<p>Next, create a policy called <code>db-policy</code> to be applied to all db containers.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl policy create db-policy
INFO[0000] Creating policy default:db-policy
vagrant@tutorial-node1:~$ netctl policy ls
Tenant   Policy
------   ------
default  db-policy
</code></pre>
<p>Populate the policy with some rules:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl policy rule-add db-policy 1 -direction=in -protocol=tcp -action=deny
vagrant@tutorial-node1:~$ netctl policy rule-add db-policy 2 -direction=in -protocol=tcp -port=8888 -action=allow -priority=10
vagrant@tutorial-node1:~$ netctl policy rule-ls db-policy
Incoming Rules:
Rule  Priority  From EndpointGroup  From Network  From IpAddress  Protocol  Port  Action
----  --------  ------------------  ------------  ---------       --------  ----  ------
1     1                                                           tcp       0     deny
2     10                                                          tcp       8888  allow
Outgoing Rules:
Rule  Priority  To EndpointGroup  To Network  To IpAddress  Protocol  Port  Action
----  --------  ----------------  ----------  ---------     --------  ----  ------
</code></pre>
<p>Finally, associate the policy with a group (a group is an arbitrary collection of 
containers) and run some containers that belong to <code>db</code> group.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ netctl group create contiv-net db -policy=db-policy
vagrant@tutorial-node1:~$ netctl group ls
Tenant   Group  Network     Policies
------   -----  -------     --------
default  db     contiv-net  db-policy

vagrant@tutorial-node1:~$ docker run -itd --name=contiv-db --net=db.contiv-net alpine /bin/sh
27eedc376ef43e5a5f3f4ede01635d447b1a0c313cca4c2a640ba4d5dea3573a
</code></pre>
<p>To verify this policy,  start the <code>netcat</code> utility to listen on an
arbitrary port within a container that belongs to <code>db</code> group. Then, from another container, 
scan a range of ports to confirm that only the one permitted by the <code>db</code> policy (in this
example, <code>port 8888</code>) is accessible on the db container.</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' contiv-db
10.1.2.7

vagrant@tutorial-node1:~$ docker exec -it contiv-db /bin/sh
/ # nc -l 8888
&lt;awaiting connection&gt;
</code></pre>
<p>Switch over to the <code>tutorial-node2</code> window, run a web container, and verify the policy:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node2:~$ docker run -itd --name=contiv-web --net=contiv-net alpine /bin/sh
54108c756699071d527a567f9b5d266284aaf5299b888d75cd19ba1a40a1135a
vagrant@tutorial-node2:~$ docker exec -it contiv-web /bin/sh

/ # nc -nzvw 1 10.1.2.7 8890
nc: 10.1.2.7 (10.1.2.7:8890): Operation timed out
/ # nc -nzvw 1 10.1.2.7 8889
nc: 10.1.2.7 (10.1.2.7:8889): Operation timed out
/ # nc -nzvw 1 10.1.2.7 8888
/ #
</code></pre>
<p>Note that the last scan on port <code>8888</code> using <code>nc -nzvw 1 10.1.2.7 8888</code> returned
without any <code>Operation timed out</code> message.</p>
<p>At this point you can add delete rules to the policy dynamically.</p>

<h2>Chapter 6: Running Containers in a Swarm Cluster</h2>
<p>This chapter demostrated how to use the swarm scheduler by 
redirecting requests to an already provisioned
swarm cluster.</p>
<p>To do so, set <code>DOCKER_HOST</code> as follows:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ export DOCKER_HOST=tcp://192.168.2.10:2375
</code></pre>
<p>Look at the status of various hosts using <code>docker info</code>:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker info
Containers: 16
Images: 3
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 2
 tutorial-node1: 192.168.2.10:2385
  └ Containers: 10
  └ Reserved CPUs: 0 / 4
  └ Reserved Memory: 0 B / 2.051 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.0.0-040000-generic, operatingsystem=Ubuntu 15.04, storagedriver=overlay
 tutorial-node2: 192.168.2.11:2385
  └ Containers: 6
  └ Reserved CPUs: 0 / 4
  └ Reserved Memory: 0 B / 2.051 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.0.0-040000-generic, operatingsystem=Ubuntu 15.04, storagedriver=overlay
CPUs: 8
Total Memory: 4.103 GiB
Name: tutorial-node1
No Proxy: 192.168.2.10,192.168.2.11,127.0.0.1,localhost,netmaster
</code></pre>
<p>The command displays details about the cluster, including number of nodes, 
number of containers in the cluster, available cpu and memory and more.</p>
<p>At this point any new containers scheduled dynamically across 
the cluster, which consists of the two nodes <code>tutorial-node1</code> and <code>tutorial-node2</code> in this tutorial.</p>
<p>Run two containers:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker run -itd --name=contiv-cluster-c1 --net=contiv-net alpine /bin/sh
f8fa35c0aa0ecd692a31e3d5249f5c158bd18902926978265acb3b38a9ed1c0d
vagrant@tutorial-node1:~$ docker run -itd --name=contiv-cluster-c2 --net=contiv-net alpine /bin/sh
bfc750736007c307827ae5012755085ca6591f3ac3ac0b707d2befd00e5d1621
</code></pre>
<p>After you start the two containers, the scheduler schedules them using the
scheduling algorithm <code>bin-packing</code> or <code>spread</code> If the two containers are not placed on 
different nodes, start more containers until the some nodes appear on both containers.</p>
<p>To see which containers get scheduled on which node, use the <code>docker ps</code> command:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker ps
CONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS                  PORTS                         NAMES
bfc750736007        alpine                         "/bin/sh"           5 seconds ago       Up Less than a second                                 tutorial-node2/contiv-cluster-c2
f8fa35c0aa0e        alpine                         "/bin/sh"           22 seconds ago      Up 16 seconds                                         tutorial-node2/contiv-cluster-c1
54108c756699        alpine                         "/bin/sh"           14 minutes ago      Up 14 minutes                                         tutorial-node2/contiv-web
27eedc376ef4        alpine                         "/bin/sh"           25 minutes ago      Up 25 minutes                                         tutorial-node1/contiv-db
1c463ecad829        alpine                         "/bin/sh"           39 minutes ago      Up 39 minutes                                         tutorial-node2/contiv-vlan-c2
4bf58874c937        alpine                         "/bin/sh"           40 minutes ago      Up 40 minutes                                         tutorial-node1/contiv-vlan-c1
52bfcc02c362        alpine                         "/bin/sh"           About an hour ago   Up About an hour        192.168.2.10:9099-&gt;9099/tcp   tutorial-node1/contiv-exposed
0fd07b44d042        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                      tutorial-node2/contiv-blue-c3
521abe19d6b5        alpine                         "/bin/sh"           About an hour ago   Up About an hour                                      tutorial-node2/contiv-blue-c2
6c7d8c0b14ec        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                            tutorial-node1/contiv-blue-c1
17afcd58b8fc        skynetservices/skydns:latest   "/skydns"           2 hours ago         Up 2 hours              53/tcp, 53/udp                tutorial-node1/bluedns
26b9f22b9790        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                            tutorial-node2/contiv-c2
46e619b0b418        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                            tutorial-node1/contiv-c1
fb822eda9916        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                            tutorial-node1/overlay-c2
0ab717006962        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                            tutorial-node1/overlay-c1
2cf083c0a4de        alpine                         "/bin/sh"           2 hours ago         Up 2 hours                                            tutorial-node1/vanilla-c
ab353464b4e2        skynetservices/skydns:latest   "/skydns"           2 hours ago         Up 2 hours              53/tcp, 53/udp                tutorial-node1/defaultdns
</code></pre>
<p>Finally, check the inter-container connectivity and external 
connectivity for the containers scheduled across multiple hosts:</p>
<pre class="highlight plaintext"><code>vagrant@tutorial-node1:~$ docker exec -it contiv-cluster-c1 /bin/sh
/ # 
/ # 
/ # 
/ # ping contiv-cluster-c2
PING contiv-cluster-c2 (10.1.2.7): 56 data bytes
64 bytes from 10.1.2.7: seq=0 ttl=64 time=8.440 ms
64 bytes from 10.1.2.7: seq=1 ttl=64 time=1.479 ms
^C
--- contiv-cluster-c2 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 1.479/4.959/8.440 ms
/ # ping contiv.com
PING contiv.com (216.239.36.21): 56 data bytes
64 bytes from 216.239.36.21: seq=0 ttl=61 time=43.537 ms
64 bytes from 216.239.36.21: seq=1 ttl=61 time=38.867 ms
^C
--- contiv.com ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 38.867/41.202/43.537 ms
</code></pre>

<h2>Clean Up</h2>
<p>To clean up after doing all the exercises, use Vagrant to
tear down the tutorial VM environment:</p>
<pre class="highlight plaintext"><code>$ vagrant destroy -f
==&gt; tutorial-node2: Forcing shutdown of VM...
==&gt; tutorial-node2: Destroying VM and associated drives...
==&gt; tutorial-node1: Forcing shutdown of VM...
==&gt; tutorial-node1: Destroying VM and associated drives...
</code></pre>

<h2>References</h2>

<ol>
<li><a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">CNI Specification</a>
</li>
<li><a href="https://github.com/docker/libnetwork/blob/master/docs/design.md">CNM Design</a>
</li>
<li><a href="http://docs.contiv.io">Contiv User Guide</a>
</li>
<li><a href="https://github.com/contiv/netplugin">Contiv Networking Code</a>
</li>
</ol>

<h2>Improvements or Comments</h2>
<p>Thank you for trying this tutorial.  The tutorial was developed by Contiv engineers, 
and we welcome your feedback.  Please file a GitHub issue to report errors or suggest
improvements, or, if you prefer, please feel free to send a pull request to the 
<a href="https://github.com/contiv/contiv.github.io.git">website repository</a>.</p>


			</div>
		</div>
	</div>
</div>