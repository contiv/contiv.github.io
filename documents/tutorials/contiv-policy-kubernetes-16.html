<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Contiv Policy Networking Tutorial (Kubernetes 1.6)">

    <link rel="shortcut icon" href="/assets/images/favicon.png">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script>
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
    <title>Contiv Policy Networking Tutorial (Kubernetes 1.6) - Contiv</title>

    <link href="/assets/stylesheets/application-ffbab1f8.css" rel="stylesheet"/>

    <!--[if lt IE 9]><script src="/assets/javascripts/html5shiv-099439cc.js"></script><script src="/assets/javascripts/respond.min-1eb35b04.js"></script><![endif]-->

    
  </head>

  <body id="page-Contiv Policy Networking Tutorial (Kubernetes 1.6)" class=" page-Contiv Policy Networking Tutorial (Kubernetes 1.6) layout-documents page-sub">

  <div class="navbar-jumbotron">
    <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/" style="color:$white"><img src="/assets/images/contivicon4-fb7a06d8.png" height="32px" width="32px" alt="Contiv logo"></a>
        </div>
        <div class="collapse navbar-collapse" id="navbar">
          <ul class="nav navbar-nav navbar-right main-nav">
            <li><a href="/documents/">Docs</a></li>
            <li><a href="https://github.com/contiv/install/releases" target="_blank">Download</a></li>
             <li><a href="/documents/support/index.html">Support</a></li>
            <li><a href="http://blogs.cisco.com/tag/contiv" target="_blank">Blog</a></li>
            
            <li><a href="https://github.com/contiv" target="_blank"><i class="fa fa-github"></i></a></li>
            <li><a href="https://contiv.herokuapp.com/" target="_blank"><i class="fa fa-slack"></i></a></li>
          </ul>
        </div>
      </div>
    </nav>
  </div>


<div class="sidebar-overlay"></div>


<aside id="sidebar" class="sidebar sidebar-default sidebar-fixed-right" role="navigation">
    
    <div class="sidebar-header header-cover">
        
        <div class="sidebar-image">
            <img src="/assets/images/logo3-a96fcb82.png" width="49px" height="56px">
        </div>
    </div>

    
    <ul class="main nav sidebar-nav">
        <li class="first"><a href="/intro/index.html">Intro</a></li>
        <li class=""><a href="/docs/index.html">Docs</a></li>
    </ul>
    <div class="divider"></div>
    
    <ul class="external nav sidebar-nav">
        <li class=""><a class="v-btn gray sml" href="https://github.com/contiv"><svg id="svg-download" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 14 14" style="enable-background:new 0 0 14 14;">
  <style>
  </style>
  <path class="" d="M13,0H1C0.5,0,0,0.5,0,1v12c0,0.5,0.5,1,1,1h4.7c0,0,0,0,0-0.1c0-0.2,0-0.6,0-1.1c-1.8,0.4-2.2-0.9-2.2-0.9
    c-0.3-0.8-0.7-1-0.7-1c-0.6-0.4,0-0.4,0-0.4c0.7,0,1,0.7,1,0.7c0.6,1,1.5,0.7,1.9,0.5c0.1-0.4,0.2-0.7,0.4-0.9c-1.5-0.2-3-0.7-3-3.2
    c0-0.7,0.3-1.3,0.7-1.8C3.7,5.8,3.5,5.1,3.9,4.2c0,0,0.6-0.2,1.8,0.7c0.5-0.1,1.1-0.2,1.6-0.2c0.6,0,1.1,0.1,1.6,0.2
    c1.3-0.8,1.8-0.7,1.8-0.7c0.4,0.9,0.1,1.6,0.1,1.7c0.4,0.5,0.7,1,0.7,1.8c0,2.5-1.5,3.1-3,3.2C8.7,11.1,9,11.5,9,12.1
    c0,0.9,0,1.6,0,1.8c0,0,0,0,0,0.1h4c0.5,0,1-0.5,1-1V1C14,0.5,13.5,0,13,0z"/>
</svg>
GitHub</a></li>
    </ul>
</aside>


<div class="container">
	<div class="row">
		<div class="col-md-3 col-sm-3 col-xs-12">
					<div class="docs-sidebar hidden-print affix-top" role="complementary">
			<hr>
			<h3>Release Notes</h3>
			<ul class="nav">
				<li>
					<a href="/documents/releasenotes/v11x.html">1.1.x</a>
					<a href="/documents/releasenotes/v10x.html">1.0.x</a>
				</li>
			</ul>
			<hr>
			<h3>Getting Started</h3>
							<ul class="nav">
							<li>
								<a href="/documents/gettingStarted/">Intro</a>
						    </li>
							<li>
								<a href="/documents/networking/features.html">Features</a>
						    </li>
								<li>
									<a href="https://github.com/contiv/install/blob/master/README.md">Installation</a>
									</li>
						    <li>
								<a href="/documents/networking/">Contiv Networking Concepts</a>
						    </li>
								</ul>
									<hr>
			<h3>Admin Guide</h3>
			<ul class="nav">
				<li>
					<a href="/documents/admin/index.html">Intro</a>
				</li>
				<li>
				    <a href="/documents/admin/createTenant.html">Managing Tenants</a>
				</li>
				<li>
				    <a href="/documents/admin/manageUsers.html">Managing Users</a>
			    </li>		
			    <li>
				    <a href="/documents/admin/manageAuthorizations.html">Managing Authorizations</a>
			    </li>
			    <li>
				    <a href="/documents/admin/createNodes.html">Managing Nodes</a>
			    </li>
			     <li>
				    <a href="/documents/admin/manageNetworks.html">Managing Networks</a>
			    </li>
			    <li>
				    <a href="/documents/admin/manageLDAP.html">Managing LDAP</a>
			    </li>
			</ul>
				</li>
			</ul>
			<hr>
			<h3>User Guide</h3>
						
					<ul class="nav">
						<li>
								<a href="/documents/networking/policies.html">Policies</a>
						</li>
						<li>
								<a href="/documents/networking/services.html">Service Loadbalancing</a>
						</li>
						<li>
								<a class="hasChildren" href="/documents/networking/physical-networks.html">Physical Networks</a>
								<ul class="nav">
									<li>
											<a href="/documents/networking/bgp.html">L3 Routed Networks</a>
									</li>
									<li>
											<a href="/documents/networking/l2-vlan.html">L2 Bridged Networks</a>
									</li>
									<li>
											<a href="/documents/networking/aci_ug.html">Cisco ACI</a>
									</li>
								</ul>
						</li>
						<li>
								<a href="/documents/networking/ipam.html">IPAM and Service Discovery</a>
						</li>
						<li>
								<a href="/documents/networking/ipv6.html">IPv6</a>
						</li>
					</ul>
				</li>
				
				
			</ul>
			<hr><p>
			<h3>Tutorials and Talks</h3>
				<ul class="nav">
					<li>
						<a href="/documents/tutorials/index.html">Tutorials</a>
					</li>
					<li>
						<a href="/documents/demos/index.html">Demonstration Videos</a>
					</li>
					<li>
						<a href="/documents/talks/index.html">Community Talks</a>
					</li>
				</ul>
			<hr><p>
			<h3>OpenShift</h3>
				<ul class="nav">
						<li>
							<a href="/documents/openshift/index.html">Contiv and OpenShift</a>
						</li>
				</ul>
			<hr><p>
			<h3>Examples</h3>
				<ul class="nav">
						</li>
						<li>
							<a href="/documents/samples/mcast.html">Multicast Examples</a>
						</li>
				</ul>
				<hr><p>
			<h3>Community</h3>
				<ul class="nav">
						</li>
						<li>
							<a href="/documents/community/index.html">Engagement</a>
							<a href="/documents/community/examples.html">Community Resources </a>
						</li>
				</ul>
			<hr><p>
			<h3>Reference</h3>
				<ul class="nav">
					    <li>
					    <a href="/documents/api/wip.html" target="_blank">API Reference</a>
					    </li> 
					    <li>
					    <a href="/documents/reference/netctlcli.html" target="_blank">CLI Reference</a>
					    </li> 
						<li><a href="https://godoc.org/github.com/contiv/contivmodel/client" target="_blank">Go Client Library</a>
						</li>
						
				</ul>
			<hr><p>
		</div>

		</div>  
		<div id="main-content" class="col-sm-9 col-md-9 col-xs-12" role="main">
			<div class="bs-docs-section">
					<h2>Contiv Policy Tutorial with Kubernetes</h2>

<ul>
<li><a href="#setup">Setup</a>
</li>
<li><a href="#ch1">Chapter 1 - ICMP Policy</a>
</li>
<li><a href="#ch2">Chapter 2 - TCP Policy</a>
</li>
<li><a href="#ch3">Chapter 3 - Bandwidth Policy</a>
</li>
<li><a href="#cleanup">Cleanup</a>
</li>
</ul>
<p>This tutorial walks through advanced features of Contiv container networking.</p>

<h3><a name="setup"></a> Setup</h3>
<p>Follow all steps from the <a href="/documents/tutorials/networking-kubernetes-16.html">Container Networking Tutorial</a>.</p>

<h3><a name="ch1"></a> Chapter 1 - ICMP Policy</h3>
<p>In this section, we will create two groups epgA and epgB. We will create containers with respect to those groups. Then, by default, communication between the groups is allowed. So, we will create an ICMP deny policy and verify that we are not able to ping between those containers.</p>
<p>Let&#39;s create a tenant and network first.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl tenant create TestTenant
Creating tenant: TestTenant 
</code></pre>
<p>Create a network under this tenant.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl network create --tenant TestTenant --subnet=10.1.1.0/24 --gateway=10.1.1.254 -e "vlan" TestNet
Creating network TestTenant:TestNet
</code></pre>
<p>We can see the networks that are present within the cluster.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl net ls -a
Tenant      Network      Nw Type  Encap type  Packet tag  Subnet        Gateway     IPv6Subnet  IPv6Gateway  Cfgd Tag
------      -------      -------  ----------  ----------  -------       ------      ----------  -----------  ---------
default     default-net  data     vxlan       0           20.1.1.0/24   20.1.1.1
...
TestTenant  TestNet      data     vlan        0           10.1.1.0/24   10.1.1.254
default     contivh1     infra    vxlan       0           132.1.1.0/24  132.1.1.1
</code></pre>
<p>Now create two network groups under network TestNet.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl group create -t TestTenant TestNet epgA
Creating EndpointGroup TestTenant:epgA 

[vagrant@kubeadm-master ~]$ netctl group create -t TestTenant TestNet epgB
Creating EndpointGroup TestTenant:epgB 
</code></pre>
<p>We can list the network groups as well.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl group ls -a
Tenant      Group  Network  IP Pool  CfgdTag  Policies  Network profile
------      -----  -------  -------  -------  --------  ---------------
TestTenant  epgA   TestNet
TestTenant  epgB   TestNet
</code></pre>
<p>Let&#39;s create two pods, one on each group network, and check whether they are able to ping each other or not. By default, Contiv allows connectivity between groups under the same network.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ cat &lt;&lt;EOF &gt; apod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: apod
  labels:
    app: apod
    io.contiv.tenant: TestTenant
    io.contiv.net-group: epgA
    io.contiv.network: TestNet
spec:
  containers:
  - name: alpine
    image: contiv/alpine
    command:
      - sleep
      - "6000"
EOF
 
[vagrant@kubeadm-master ~]$ kubectl create -f apod.yaml
pod "apod" created
</code></pre>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ cat &lt;&lt;EOF &gt; bpod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: bpod
  labels:
    app: bpod
    io.contiv.tenant: TestTenant
    io.contiv.net-group: epgB
    io.contiv.network: TestNet
spec:
  containers:
  - name: alpine
    image: contiv/alpine
    command:
      - sleep
      - "6000"
EOF
 
[vagrant@kubeadm-master ~]$ kubectl create -f bpod.yaml
pod "bpod" created
</code></pre>
<p>Let&#39;s make sure our pods are up and running.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ kubectl get pods -o wide
NAME                        READY     STATUS    RESTARTS   AGE       IP         NODE
apod                        1/1       Running   0          35s       10.1.1.1   kubeadm-worker0
bpod                        1/1       Running   0          19s       10.1.1.2   kubeadm-worker0
...
</code></pre>
<p>Now try to ping from apod to bpod. They should be able to ping each other.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ kubectl exec -it bpod sh
/ # ifconfig
eth0      Link encap:Ethernet  HWaddr 02:02:0A:01:01:02
          inet addr:10.1.1.2  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::2:aff:fe01:102/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1
          RX packets:8 errors:0 dropped:0 overruns:0 frame:0
          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)

/ # exit
</code></pre>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ kubectl exec -it apod sh
/ # ifconfig
eth0      Link encap:Ethernet  HWaddr 02:02:0A:01:01:01
          inet addr:10.1.1.1  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::2:aff:fe01:101/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1
          RX packets:8 errors:0 dropped:0 overruns:0 frame:0
          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)

/ # ping -c 3 10.1.1.2
PING 10.1.1.2 (10.1.1.2): 56 data bytes
64 bytes from 10.1.1.2: seq=0 ttl=64 time=46.369 ms
64 bytes from 10.1.1.2: seq=1 ttl=64 time=0.085 ms
64 bytes from 10.1.1.2: seq=2 ttl=64 time=0.076 ms

--- 10.1.1.2 ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 0.076/15.510/46.369 ms
/ # exit
</code></pre>
<p>Now let’s add an ICMP Deny policy and modify group epgB. The pods should not be able to ping each other now.</p>
<p>Create the policy.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl policy create -t TestTenant policyAB
Creating policy TestTenant:policyAB 
</code></pre>
<p>Add a rule to the policy.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl policy rule-add -t TestTenant -d in --protocol icmp --from-group epgA --action deny policyAB 1
</code></pre>
<p>Create a group associated with this policy.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl group create -t TestTenant -p policyAB TestNet epgB
Creating EndpointGroup TestTenant:epgB 
</code></pre>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl policy ls -a
Tenant      Policy
------      ------
TestTenant  policyAB
 
[vagrant@kubeadm-master ~]$ netctl policy rule-ls -t TestTenant policyAB
Incoming Rules:
Rule  Priority  From EndpointGroup  From Network  From IpAddress  Protocol  Port  Action
----  --------  ------------------  ------------  ---------       --------  ----  ------
1     1         epgA                                              icmp      0     deny
Outgoing Rules:
Rule  Priority  To EndpointGroup  To Network  To IpAddress  Protocol  Port  Action
----  --------  ----------------  ----------  ---------     --------  ----  ------
</code></pre>
<p>Now ping between pods should not work.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ kubectl exec -it apod sh
/ # ping -c 3 10.1.1.2
PING 10.1.1.2 (10.1.1.2): 56 data bytes

--- 10.1.1.2 ping statistics ---
3 packets transmitted, 0 packets received, 100% packet loss
/ # exit
</code></pre>

<h3><a name="ch2"></a> Chapter 2 - TCP Policy</h3>
<p>In this section, we will create a TCP deny policy as well as a selective TCP port allow policy.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl policy rule-add -t TestTenant -d in --protocol tcp --port 0  --from-group epgA  --action deny policyAB 2
[vagrant@kubeadm-master ~]$ netctl policy rule-add -t TestTenant -d in --protocol tcp --port 8001  --from-group epgA  --action allow --priority 10 policyAB 3
[vagrant@kubeadm-master ~]$ netctl policy rule-ls -t TestTenant policyAB
Incoming Rules:
Rule  Priority  From EndpointGroup  From Network  From IpAddress  Protocol  Port  Action
----  --------  ------------------  ------------  ---------       --------  ----  ------
1     1         epgA                                              icmp      0     deny
2     1         epgA                                              tcp       0     deny
3     10        epgA                                              tcp       8001  allow
Outgoing Rules:
Rule  Priority  To EndpointGroup  To Network  To IpAddress  Protocol  Port  Action
----  --------  ----------------  ----------  ---------     --------  ----  ------
</code></pre>
<p>Now check that from epgB, only TCP 8001 port is open. To test this, let&#39;s run <code>iperf</code> on bpod and verify using the <code>nc</code> utility on apod.</p>
<p>On bpod:</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ kubectl exec -it bpod sh
/ # iperf -s -p 8001
------------------------------------------------------------
Server listening on TCP port 8001
TCP window size: 85.3 KByte (default)
------------------------------------------------------------
</code></pre>
<p>Open another terminal and login to apod:</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ kubectl exec -it apod sh
/ # nc -zvw 1 10.1.1.2 8001     # here 10.1.1.2 is IP address of bpod.
10.1.1.2 (10.1.1.2:8001) open
/ # nc -zvw 1 10.1.1.2 8000
nc: 10.1.1.2 (10.1.1.2:8000): Operation timed out
/ # exit
</code></pre>
<p>We can see that port 8001 is open and port 8000 is not open. Exit bpod as well.</p>
<pre class="highlight plaintext"><code>/ # iperf -s -p 8001
------------------------------------------------------------
Server listening on TCP port 8001
TCP window size: 85.3 KByte (default)
------------------------------------------------------------
[  4] local 10.1.1.2 port 8001 connected with 10.1.1.1 port 38137
[ ID] Interval       Transfer     Bandwidth
[  4]  0.0- 0.0 sec  0.00 Bytes  0.00 bits/sec
^C/ # exit
</code></pre>

<h3><a name="ch3"></a> Chapter 3 - Bandwidth Policy</h3>
<p>In this chapter, we will explore the bandwidth policy feature of Contiv. We will create a tenant, a network and some groups. Then we will attach a netprofile to one endpoint group and verify that the applied bandwidth policy works as expected.</p>
<p>So, let&#39;s create a tenant, a network and group &quot;A&quot; under the network.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl tenant create BandwidthTenant
Creating tenant: BandwidthTenant
[vagrant@kubeadm-master ~]$ netctl network create --tenant BandwidthTenant --subnet=50.1.1.0/24 --gateway=50.1.1.254 -p 1001 -e "vlan" BandwidthTestNet
Creating network BandwidthTenant:BandwidthTestNet
[vagrant@kubeadm-master ~]$ netctl group create -t BandwidthTenant BandwidthTestNet epgA
Creating EndpointGroup BandwidthTenant:epgA
</code></pre>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl net ls -a
Tenant           Network           Nw Type  Encap type  Packet tag  Subnet        Gateway     IPv6Subnet  IPv6Gateway  Cfgd Tag
------           -------           -------  ----------  ----------  -------       ------      ----------  -----------  ---------
...
TestTenant       TestNet           data     vlan        0           10.1.1.0/24   10.1.1.254
BandwidthTenant  BandwidthTestNet  data     vlan        1001        50.1.1.0/24   50.1.1.254
...
</code></pre>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl group ls -a
Tenant           Group  Network           IP Pool  CfgdTag  Policies  Network profile
------           -----  -------           -------  -------  --------  ---------------
TestTenant       epgA   TestNet
TestTenant       epgB   TestNet                             policyAB
BandwidthTenant  epgA   BandwidthTestNet
</code></pre>
<p>Now, we are going to run two containers in the epgA network space: aserver and aclient.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ cat &lt;&lt;EOF &gt; aserver.yaml
apiVersion: v1
kind: Pod
metadata:
  name: aserver
  labels:
    app: aserver
    io.contiv.tenant: BandwidthTenant
    io.contiv.net-group: epgA
    io.contiv.network: BandwidthTestNet
spec:
  containers:
  - name: alpine
    image: contiv/alpine
    command:
      - sleep
      - "6000"
EOF

[vagrant@kubeadm-master ~]$ kubectl create -f aserver.yaml
pod "aserver" created
</code></pre>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ cat &lt;&lt;EOF &gt; aclient.yaml
apiVersion: v1
kind: Pod
metadata:
  name: aclient
  labels:
    app: aclient
    io.contiv.tenant: BandwidthTenant
    io.contiv.net-group: epgA
    io.contiv.network: BandwidthTestNet
spec:
  containers:
  - name: alpine
    image: contiv/alpine
    command:
      - sleep
      - "6000"
EOF

[vagrant@kubeadm-master ~]$ kubectl create -f aclient.yaml
pod "aclient" created
</code></pre>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ kubectl get pods
NAME                        READY     STATUS    RESTARTS   AGE
aclient                     1/1       Running   0          6s
apod                        1/1       Running   0          13m
aserver                     1/1       Running   0          23s
bpod                        1/1       Running   0          13m
...
</code></pre>
<p>Now run <code>iperf</code> on the server and client to find out the current bandwidth policies which are on the network. It may vary depending upon base OS, network speed, etc.</p>
<p>On aserver:</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ kubectl exec -it aserver sh
/ # ifconfig
eth0      Link encap:Ethernet  HWaddr 02:02:32:01:01:01
          inet addr:50.1.1.1  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::2:32ff:fe01:101/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1
          RX packets:8 errors:0 dropped:0 overruns:0 frame:0
          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)

/ # iperf -s -u
------------------------------------------------------------
Server listening on UDP port 5001
Receiving 1470 byte datagrams
UDP buffer size:  208 KByte (default)
------------------------------------------------------------
</code></pre>
<p>Open up a new terminal and login to aclient:</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ kubectl exec -it aclient sh
/ # iperf -c 50.1.1.1 -u
------------------------------------------------------------
Client connecting to 50.1.1.1, UDP port 5001
Sending 1470 byte datagrams, IPG target: 11215.21 us (kalman adjust)
UDP buffer size:  208 KByte (default)
------------------------------------------------------------
[  3] local 50.1.1.2 port 46490 connected with 50.1.1.1 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  1.25 MBytes  1.05 Mbits/sec
[  3] Sent 893 datagrams
[  3] Server Report:
[  3]  0.0-10.0 sec  1.25 MBytes  1.05 Mbits/sec   0.008 ms    0/  893 (0%)
/ # exit
</code></pre>
<p>Exit aserver.</p>
<pre class="highlight plaintext"><code>/ # iperf -s -u
------------------------------------------------------------
Server listening on UDP port 5001
Receiving 1470 byte datagrams
UDP buffer size:  208 KByte (default)
------------------------------------------------------------
[  3] local 50.1.1.1 port 5001 connected with 50.1.1.2 port 46490
[ ID] Interval       Transfer     Bandwidth        Jitter   Lost/Total Datagrams
[  3]  0.0-10.0 sec  1.25 MBytes  1.05 Mbits/sec   0.009 ms    0/  893 (0%)
^C/ # exit
</code></pre>
<p>Now we see that the current bandwidth we are getting is 1.05 Mbits/sec. So let&#39;s create a new group B and create a netprofile with a bandwidth less than the one we got above: 500Kbits/sec bandwidth.</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ netctl netprofile create -t BandwidthTenant -b 500Kbps -d 6 -s 80 testProfile
Creating netprofile BandwidthTenant:testProfile

[vagrant@kubeadm-master ~]$ netctl group create -t BandwidthTenant -n testProfile BandwidthTestNet epgB
Creating EndpointGroup BandwidthTenant:epgB

[vagrant@kubeadm-master ~]$ netctl netprofile ls -a
Name         Tenant           Bandwidth  DSCP      burst size
------       ------           ---------  --------  ----------
testProfile  BandwidthTenant  500Kbps    6         80

[vagrant@kubeadm-master ~]$ netctl group ls -a
Tenant           Group  Network           IP Pool  CfgdTag  Policies  Network profile
------           -----  -------           -------  -------  --------  ---------------
TestTenant       epgA   TestNet
TestTenant       epgB   TestNet                             policyAB
BandwidthTenant  epgA   BandwidthTestNet
BandwidthTenant  epgB   BandwidthTestNet                              testProfile
</code></pre>
<p>Run bclient and bserver pods:</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ cat &lt;&lt;EOF &gt; bserver.yaml
apiVersion: v1
kind: Pod
metadata:
  name: bserver
  labels:
    app: bserver
    io.contiv.tenant: BandwidthTenant
    io.contiv.net-group: epgB
    io.contiv.network: BandwidthTestNet
spec:
  containers:
  - name: alpine
    image: contiv/alpine
    command:
      - sleep
      - "6000"
EOF

[vagrant@kubeadm-master ~]$ kubectl create -f bserver.yaml
pod "bserver" created
</code></pre>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ cat &lt;&lt;EOF &gt; bclient.yaml
apiVersion: v1
kind: Pod
metadata:
  name: bclient
  labels:
    app: bclient
    io.contiv.tenant: BandwidthTenant
    io.contiv.net-group: epgB
    io.contiv.network: BandwidthTestNet
spec:
  containers:
  - name: alpine
    image: contiv/alpine
    command:
      - sleep
      - "6000"
EOF

[vagrant@kubeadm-master ~]$ kubectl create -f bclient.yaml
pod "bclient" created
</code></pre>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ kubectl get pods
NAME                        READY     STATUS    RESTARTS   AGE
aclient                     1/1       Running   0          15m
apod                        1/1       Running   0          28m
aserver                     1/1       Running   0          15m
bclient                     1/1       Running   0          1m
bpod                        1/1       Running   0          28m
bserver                     1/1       Running   0          1m
...
</code></pre>
<p>Now we are running bclient and bserver pods on the group B network. We should see bandwidth around 500Kbps when we run <code>iperf</code>. Let&#39;s verify that our bandwidth netprofile is working as expected.</p>
<p>On bserver:</p>
<pre class="highlight plaintext"><code>/ # ifconfig
eth0      Link encap:Ethernet  HWaddr 02:02:32:01:01:03
          inet addr:50.1.1.3  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::2:32ff:fe01:103/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1
          RX packets:8 errors:0 dropped:0 overruns:0 frame:0
          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)

/ # iperf -s -u
------------------------------------------------------------
Server listening on UDP port 5001
Receiving 1470 byte datagrams
UDP buffer size:  208 KByte (default)
------------------------------------------------------------
</code></pre>
<p>Open another terminal and login to bclient:</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ kubectl exec -it bclient sh
/ # iperf -c 50.1.1.3 -u
------------------------------------------------------------
Client connecting to 50.1.1.3, UDP port 5001
Sending 1470 byte datagrams, IPG target: 11215.21 us (kalman adjust)
UDP buffer size:  208 KByte (default)
------------------------------------------------------------
[  3] local 50.1.1.4 port 53775 connected with 50.1.1.3 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  1.25 MBytes  1.05 Mbits/sec
[  3] Sent 893 datagrams
[  3] Server Report:
[  3]  0.0-10.0 sec   620 KBytes   509 Kbits/sec   0.008 ms  461/  893 (52%)
/ # exit

</code></pre>
<p>Exit bserver.</p>
<pre class="highlight plaintext"><code>/ # iperf -s -u
------------------------------------------------------------
Server listening on UDP port 5001
Receiving 1470 byte datagrams
UDP buffer size:  208 KByte (default)
------------------------------------------------------------
[  3] local 50.1.1.3 port 5001 connected with 50.1.1.4 port 53775
[ ID] Interval       Transfer     Bandwidth        Jitter   Lost/Total Datagrams
[  3]  0.0-10.0 sec   620 KBytes   509 Kbits/sec   0.009 ms  461/  893 (52%)
^C/ # exit
</code></pre>
<p>We can see that bclient is getting roughly around 500Kbps bandwidth.</p>

<h3><a name="cleanup"></a> Cleanup</h3>
<p>To cleanup the setup, after doing all the experiments, exit the VM and destroy the VMs:</p>
<pre class="highlight plaintext"><code>[vagrant@kubeadm-master ~]$ exit
logout
Connection to 127.0.0.1 closed.
</code></pre>
<pre class="highlight plaintext"><code>$ cd .. # go back to install directory
$ make cluster-destroyi
cd cluster &amp;&amp; vagrant destroy -f
==&gt; kubeadm-worker0: Forcing shutdown of VM...
==&gt; kubeadm-worker0: Destroying VM and associated drives...
==&gt; kubeadm-master: Forcing shutdown of VM...
==&gt; kubeadm-master: Destroying VM and associated drives...
==&gt; swarm-mode-worker0: VM not created. Moving on...
==&gt; swarm-mode-master: VM not created. Moving on...
==&gt; legacy-swarm-worker0: VM not created. Moving on...
==&gt; legacy-swarm-master: VM not created. Moving on...
</code></pre>
<pre class="highlight plaintext"><code>$ make vagrant-clean
vagrant global-status --prune
id       name   provider state  directory
--------------------------------------------------------------------
There are no active Vagrant environments on this computer! Or,
you haven't destroyed and recreated Vagrant environments that were
started with an older version of Vagrant.
cd cluster &amp;&amp; vagrant destroy -f
==&gt; kubeadm-worker0: VM not created. Moving on...
==&gt; kubeadm-master: VM not created. Moving on...
==&gt; swarm-mode-worker0: VM not created. Moving on...
==&gt; swarm-mode-master: VM not created. Moving on...
==&gt; legacy-swarm-worker0: VM not created. Moving on...
==&gt; legacy-swarm-master: VM not created. Moving on...
</code></pre>

<h3>References</h3>

<ol>
<li><a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">CNI Specification</a>
</li>
<li><a href="https://github.com/docker/libnetwork/blob/master/docs/design.md">CNM Design</a>
</li>
<li><a href="http://docs.contiv.io">Contiv User Guide</a>
</li>
<li><a href="https://github.com/contiv/netplugin">Contiv Networking Code</a>
</li>
</ol>

<h3>Improvements or Comments</h3>
<p>This tutorial was developed by Contiv engineers. Thank you for trying out this tutorial.
Please file a GitHub issue if you see an issue with the tutorial, or if you prefer
improving some text, feel free to send a pull request.</p>

	<hr>

			</div>
		</div>
	</div>
</div>


<div class="row">
    <div class="col-md-10 col-md-offset-1">
        <hr>
            <p style="text-align: center"><font color="black">Copyright &copy; 2017 Cisco and/or its affiliates. All rights reserved.</font>
            </p>
    </div>
</div>